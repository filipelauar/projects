{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gridworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"gridworld-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.setPlan(\"gridworldPlans\\\\plan1.txt\",{0:-0.001,3:1,4:1,5:-1,6:-1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, P = env.getMDP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon: 0.001 Gamma: 0.999\n",
      "Number of iterations: 118\n",
      "\n",
      "Epsilon: 0.001 Gamma: 0.99\n",
      "Number of iterations: 40\n",
      "\n",
      "Epsilon: 0.001 Gamma: 0.9\n",
      "Number of iterations: 28\n",
      "\n",
      "Epsilon: 0.01 Gamma: 0.999\n",
      "Number of iterations: 66\n",
      "\n",
      "Epsilon: 0.01 Gamma: 0.99\n",
      "Number of iterations: 35\n",
      "\n",
      "Epsilon: 0.01 Gamma: 0.9\n",
      "Number of iterations: 23\n",
      "\n",
      "Epsilon: 0.1 Gamma: 0.999\n",
      "Number of iterations: 31\n",
      "\n",
      "Epsilon: 0.1 Gamma: 0.99\n",
      "Number of iterations: 29\n",
      "\n",
      "Epsilon: 0.1 Gamma: 0.9\n",
      "Number of iterations: 17\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def val_iter(env = env, eps=.0001, gamma = .9, max_iter = 500):\n",
    "    states, P = env.getMDP()\n",
    "    ps = [p for p in P]\n",
    "    v1, v0 = np.zeros(len(states)), np.ones(len(states))\n",
    "    t = 0\n",
    "    pol = [0]*len(states)\n",
    "    while np.sum(np.abs(v1-v0)) > eps and t < max_iter:\n",
    "        v0=v1.copy()\n",
    "        t+=1\n",
    "        for i in ps:\n",
    "            best_reward = -2\n",
    "            for act in range(env.nA):\n",
    "                reward = 0\n",
    "                for state_probability, next_state, next_reward, terminated  in P[i][act]:\n",
    "                    reward = reward + state_probability*(next_reward + gamma*v0[next_state])\n",
    "                if reward > best_reward:\n",
    "                    best_reward=reward\n",
    "                    pol[i] = act\n",
    "                \n",
    "            v1[i] = best_reward\n",
    "            \n",
    "    print(\"Number of iterations:\", t)\n",
    "    return v1, pol\n",
    "\n",
    "\n",
    "epsilon = [0.001, 0.01, 0.1]\n",
    "gamma = [1 - e for e in epsilon]\n",
    "    \n",
    "for e in epsilon:\n",
    "    for g in gamma:\n",
    "        print(\"Epsilon:\", e, \"Gamma:\", g)\n",
    "        v1, pol = val_iter(eps=e, gamma=g)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Police evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon: 0.001 Gamma: 0.999\n",
      "Number of iterations: 4\n",
      "\n",
      "Epsilon: 0.001 Gamma: 0.99\n",
      "Number of iterations: 3\n",
      "\n",
      "Epsilon: 0.001 Gamma: 0.9\n",
      "Number of iterations: 3\n",
      "\n",
      "Epsilon: 0.01 Gamma: 0.999\n",
      "Number of iterations: 4\n",
      "\n",
      "Epsilon: 0.01 Gamma: 0.99\n",
      "Number of iterations: 3\n",
      "\n",
      "Epsilon: 0.01 Gamma: 0.9\n",
      "Number of iterations: 3\n",
      "\n",
      "Epsilon: 0.1 Gamma: 0.999\n",
      "Number of iterations: 500\n",
      "\n",
      "Epsilon: 0.1 Gamma: 0.99\n",
      "Number of iterations: 500\n",
      "\n",
      "Epsilon: 0.1 Gamma: 0.9\n",
      "Number of iterations: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"gridworld-v0\")\n",
    "env.setPlan(\"gridworldPlans\\\\plan0.txt\",{0:-0.001,3:1,4:1,5:-1,6:-1})\n",
    "states, P = env.getMDP()\n",
    "\n",
    "\n",
    "def pol_eval(pol, env, max_iter=500, eps=.01, gamma = .99):\n",
    "    states, P = env.getMDP()\n",
    "    ps = [p for p in P]\n",
    "    v1, v0 = np.zeros(len(states)), np.ones(len(states))\n",
    "    t = 0\n",
    "    while np.sum(np.abs(v1-v0)) > eps and t < max_iter:\n",
    "        v0=v1.copy()\n",
    "        t+=1\n",
    "        for i in ps:\n",
    "            r=0\n",
    "            for action, action_probability in enumerate(pol[i]):\n",
    "                for state_probability, next_state, reward, terminated in P[i][action]:\n",
    "                     r+= action_probability * state_probability * (reward + gamma * v0[next_state])\n",
    "            v1[i] = r\n",
    "                       \n",
    "            \n",
    "    return v1\n",
    "\n",
    "\n",
    "\n",
    "def one_step_lookahead(env, state, V, gamma = .9):\n",
    "    P = env.P\n",
    "    act_val = np.zeros(env.nA)\n",
    "    for act in range(env.nA):\n",
    "        r=0\n",
    "        for state_probability, next_state, reward, terminated in P[state][act]:\n",
    "            r+= state_probability * (reward + gamma * V[next_state])\n",
    "        act_val[act] = r\n",
    "            \n",
    "    return act_val\n",
    "\n",
    "\n",
    "def pol_iter(env = env, eps=.01, gamma = .9, max_iter=500):\n",
    "    states, P = env.getMDP()\n",
    "    ps = [p for p in P]\n",
    "    pol0, pol1 = np.zeros([env.nS, env.nA]) / env.nA, np.ones([env.nS, env.nA]) / env.nA\n",
    "    # Initialize counter of evaluated policies\n",
    "    t = 0\n",
    "    while (pol0 != pol1).any() and t < max_iter:\n",
    "        pol0 = pol1.copy()\n",
    "        V = pol_eval(pol1, env, max_iter, eps, gamma)\n",
    "        \n",
    "        for i in ps:\n",
    "            current_action = np.argmax(pol1[i])\n",
    "            action_value = one_step_lookahead(env, i, V, gamma)\n",
    "            best_action = np.argmax(action_value)\n",
    "            #print(action_value)\n",
    "            if current_action != best_action:\n",
    "                pol1[i] = np.eye(env.nA)[best_action]\n",
    "        t += 1\n",
    "        \n",
    "    print(\"Number of iterations:\", t)\n",
    "    \n",
    "    return V, np.argmax(pol1, axis=1)\n",
    "\n",
    "#V, pol = pol_iter()\n",
    "epsilon = [0.001, 0.01, 0.1]\n",
    "gamma = [1 - e for e in epsilon]\n",
    "#epsilon = [0.001, 0.01]\n",
    "for e in epsilon:\n",
    "    for g in gamma:\n",
    "        print(\"Epsilon:\", e, \"Gamma:\", g)\n",
    "        v1, pol = pol_iter(eps=e, gamma=g)\n",
    "        print('')\n",
    "#V, pol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\n",
      "\u001b[40m \u001b[0m\u001b[47m \u001b[0m\u001b[47m \u001b[0m\u001b[47m \u001b[0m\u001b[42m \u001b[0m\u001b[40m \u001b[0m\n",
      "\u001b[40m \u001b[0m\u001b[47m \u001b[0m\u001b[40m \u001b[0m\u001b[47m \u001b[0m\u001b[41m \u001b[0m\u001b[40m \u001b[0m\n",
      "\u001b[40m \u001b[0m\u001b[47m \u001b[0m\u001b[47m \u001b[0m\u001b[47m \u001b[0m\u001b[44m \u001b[0m\u001b[40m \u001b[0m\n",
      "\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\n",
      "\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\n",
      "Nombre d'etats :  11\n",
      "done\n",
      "Epsilon: 0.001 Gamma: 0.999\n",
      "Number of iterations: 51\n",
      "\n",
      "Average reward: 0.980483\n",
      "Average nb actions: 20.517\n",
      "\n",
      "Epsilon: 0.001 Gamma: 0.99\n",
      "Number of iterations: 46\n",
      "\n",
      "Average reward: 0.980839\n",
      "Average nb actions: 20.161\n",
      "\n",
      "Epsilon: 0.001 Gamma: 0.9\n",
      "Number of iterations: 16\n",
      "\n",
      "Average reward: 0.542124\n",
      "Average nb actions: 4.876\n",
      "\n",
      "Epsilon: 0.01 Gamma: 0.999\n",
      "Number of iterations: 31\n",
      "\n",
      "Average reward: 0.981018\n",
      "Average nb actions: 19.982\n",
      "\n",
      "Epsilon: 0.01 Gamma: 0.99\n",
      "Number of iterations: 27\n",
      "\n",
      "Average reward: 0.980223\n",
      "Average nb actions: 20.777\n",
      "\n",
      "Epsilon: 0.01 Gamma: 0.9\n",
      "Number of iterations: 12\n",
      "\n",
      "Average reward: 0.566209\n",
      "Average nb actions: 4.791\n",
      "\n",
      "Epsilon: 0.1 Gamma: 0.999\n",
      "Number of iterations: 13\n",
      "\n",
      "Average reward: 0.7447429999999999\n",
      "Average nb actions: 10.257\n",
      "\n",
      "Epsilon: 0.1 Gamma: 0.99\n",
      "Number of iterations: 13\n",
      "\n",
      "Average reward: 0.7768740000000001\n",
      "Average nb actions: 10.126\n",
      "\n",
      "Epsilon: 0.1 Gamma: 0.9\n",
      "Number of iterations: 9\n",
      "\n",
      "Average reward: 0.47659500000000005\n",
      "Average nb actions: 4.405\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"gridworld-v0\")\n",
    "\n",
    "\n",
    "env.setPlan(\"gridworldPlans/plan0.txt\", {0: -0.001, 3: 1, 4: 1, 5: -1, 6: -1})\n",
    "\n",
    "env.seed(0)  # Initialise le seed du pseudo-random\n",
    "env.render(mode=\"human\") #visualisation sur la console\n",
    "states, mdp = env.getMDP()  # recupere le mdp et la liste d'etats\n",
    "print(\"Nombre d'etats : \",len(states))\n",
    "print(\"done\")\n",
    "state, transitions = list(mdp.items())[0]\n",
    "\n",
    "episode_count = 1000\n",
    "reward = 0\n",
    "done = False\n",
    "rsum = 0\n",
    "\n",
    "re = []\n",
    "actions = []\n",
    "epsilon = [0.001, 0.01, 0.1]\n",
    "gamma = [1 - e for e in epsilon]\n",
    "\n",
    "for e in epsilon:\n",
    "    for g in gamma:\n",
    "        re = []\n",
    "        actions = []\n",
    "        \n",
    "        print(\"Epsilon:\", e, \"Gamma:\", g)\n",
    "        v1, pol = val_iter(env=env, eps=e, gamma=g)\n",
    "        #v1, pol = pol_iter(env=env, eps=e, gamma=g)\n",
    "        print('')\n",
    "\n",
    "        for i in range(episode_count):\n",
    "            obs = env.reset()\n",
    "            env.verbose = 0\n",
    "            if env.verbose:\n",
    "                env.render()\n",
    "\n",
    "            j = 0\n",
    "            rsum = 0\n",
    "            while True:\n",
    "                action = pol[env.getStateFromObs(obs)]\n",
    "\n",
    "                obs, reward, done, _ = env.step(action)\n",
    "                rsum += reward\n",
    "                j += 1\n",
    "                if env.verbose:\n",
    "                    env.render()\n",
    "                if done:\n",
    "                    #print(\"Episode : \" + str(i) + \" rsum=\" + str(rsum) + \", \" + str(j) + \" actions\")\n",
    "                    re.append(rsum)\n",
    "                    actions.append(j)\n",
    "                    env.reset()\n",
    "                    break\n",
    "\n",
    "        print(\"Average reward:\", np.mean(re))\n",
    "        print(\"Average nb actions:\", np.mean(actions))\n",
    "        print('')\n",
    "        env.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\n",
      "\u001b[40m \u001b[0m\u001b[47m \u001b[0m\u001b[47m \u001b[0m\u001b[47m \u001b[0m\u001b[40m \u001b[0m\u001b[47m \u001b[0m\u001b[47m \u001b[0m\u001b[40m \u001b[0m\n",
      "\u001b[40m \u001b[0m\u001b[47m \u001b[0m\u001b[47m \u001b[0m\u001b[47m \u001b[0m\u001b[40m \u001b[0m\u001b[47m \u001b[0m\u001b[47m \u001b[0m\u001b[40m \u001b[0m\n",
      "\u001b[40m \u001b[0m\u001b[47m \u001b[0m\u001b[44m \u001b[0m\u001b[47m \u001b[0m\u001b[47m \u001b[0m\u001b[47m \u001b[0m\u001b[47m \u001b[0m\u001b[40m \u001b[0m\n",
      "\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[47m \u001b[0m\u001b[40m \u001b[0m\n",
      "\u001b[40m \u001b[0m\u001b[47m \u001b[0m\u001b[47m \u001b[0m\u001b[47m \u001b[0m\u001b[40m \u001b[0m\u001b[47m \u001b[0m\u001b[47m \u001b[0m\u001b[40m \u001b[0m\n",
      "\u001b[40m \u001b[0m\u001b[47m \u001b[0m\u001b[46m \u001b[0m\u001b[47m \u001b[0m\u001b[47m \u001b[0m\u001b[47m \u001b[0m\u001b[42m \u001b[0m\u001b[40m \u001b[0m\n",
      "\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\n",
      "Jeu: gridworldPlans/plan7.txt\n",
      "Methode: val\n",
      "Number of iterations: 298\n",
      "Average reward: 1.2855890000000736\n",
      "Average nb actions: 696.721\n",
      "\n",
      "Jeu: gridworldPlans/plan7.txt\n",
      "Methode: pol\n",
      "Number of iterations: 10\n",
      "Average reward: 0.7960590000000698\n",
      "Average nb actions: 647.79\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"gridworld-v0\")\n",
    "\n",
    "\n",
    "env.setPlan(\"gridworldPlans/plan2.txt\", {0: -0.001, 3: 1, 4: 1, 5: -1, 6: -1})\n",
    "\n",
    "env.seed(0)  # Initialise le seed du pseudo-random\n",
    "env.render(mode=\"human\") #visualisation sur la console\n",
    "states, mdp = env.getMDP()  # recupere le mdp et la liste d'etats\n",
    "state, transitions = list(mdp.items())[0]\n",
    "\n",
    "episode_count = 1000\n",
    "reward = 0\n",
    "done = False\n",
    "rsum = 0\n",
    "\n",
    "re = []\n",
    "actions = []\n",
    "epsilon = 0.001\n",
    "gamma = 0.999\n",
    "\n",
    "\n",
    "\n",
    "for plan in range(7,8):\n",
    "    for methode in ['val', 'pol']:\n",
    "        if plan != 9:\n",
    "            jeu = 'gridworldPlans/plan' + str(plan) + '.txt'\n",
    "            env.setPlan(jeu, {0: -0.001, 3: 1, 4: 1, 5: -1, 6: -1})\n",
    "            re = []\n",
    "            actions = []\n",
    "            #print(\"Epsilon:\", e, \"Gamma:\", g)\n",
    "            print(\"Jeu:\", jeu)\n",
    "            print(\"Methode:\", methode)\n",
    "            if methode == 'val':\n",
    "                v1, pol = val_iter(env=env, eps=epsilon, gamma=gamma)\n",
    "            else:\n",
    "                v1, pol = pol_iter(env=env, eps=epsilon, gamma=gamma)\n",
    "\n",
    "            for i in range(episode_count):\n",
    "                obs = env.reset()\n",
    "                env.verbose = 0\n",
    "                if env.verbose:\n",
    "                    env.render()\n",
    "\n",
    "                j = 0\n",
    "                rsum = 0\n",
    "                while True:\n",
    "                    action = pol[env.getStateFromObs(obs)]\n",
    "\n",
    "                    obs, reward, done, _ = env.step(action)\n",
    "                    rsum += reward\n",
    "                    j += 1\n",
    "                    if env.verbose:\n",
    "                        env.render()\n",
    "                    if done:\n",
    "                        #print(\"Episode : \" + str(i) + \" rsum=\" + str(rsum) + \", \" + str(j) + \" actions\")\n",
    "                        re.append(rsum)\n",
    "                        actions.append(j)\n",
    "                        env.reset()\n",
    "                        break\n",
    "\n",
    "            print(\"Average reward:\", np.mean(re))\n",
    "            print(\"Average nb actions:\", np.mean(actions))\n",
    "            print('')\n",
    "            env.close();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
