{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "StyleGAN_Colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5vmoWPhHjRB"
      },
      "source": [
        "#**StyleGAN Project**\n",
        "Authors:\n",
        "\n",
        "Brayam Castillo, Gabriel Baker, Filipe Lauar, Vin√≠cius Imaizumi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1Kfn3aRIAPz"
      },
      "source": [
        "Note:\n",
        "\n",
        "Most of the code was about using the github repositories, without much \"original\" code by us.\n",
        "\n",
        "Also, we had really a lot of problems trying to use Caffe in Colab. We sucessed only when installing locally, thus the code using the FaceAttribute-FAN isn't here. We only used the network to predict images, without changing its code.\n",
        "\n",
        "The function used to plot the graphs from the output of FaceAttribute-FAN was defined in the last cell of this Colab, but we used it only locally in the computer with Caffe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQxJ1dqyJWfs"
      },
      "source": [
        "## StyleGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWAHgPpXyRN1",
        "outputId": "2073c744-d058-4fd7-e361-cb771cbeac6b"
      },
      "source": [
        "%tensorflow_version 1.x # required to use StyleGAN\n",
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.x # required to use StyleGAN`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n",
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMfiSDTNyTQE",
        "outputId": "a5f2e2bf-c7d1-4146-fce1-347726854da7"
      },
      "source": [
        "! git clone https://github.com/NVlabs/stylegan"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'stylegan'...\n",
            "remote: Enumerating objects: 86, done.\u001b[K\n",
            "remote: Total 86 (delta 0), reused 0 (delta 0), pack-reused 86\u001b[K\n",
            "Unpacking objects: 100% (86/86), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2PxAfuCzOmj"
      },
      "source": [
        "import os"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYHHqOLfzm92"
      },
      "source": [
        "os.chdir(\"stylegan\")  # changing working dir to do the imports properly"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96r46i-TSwlZ",
        "outputId": "ab9d0852-5a39-44b4-f038-437cd7178bbc"
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import dnnlib as dnnlib\n",
        "import dnnlib.tflib as tflib\n",
        "import config as config"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/stylegan/dnnlib/tflib/tfutil.py:34: The name tf.Dimension is deprecated. Please use tf.compat.v1.Dimension instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/stylegan/dnnlib/tflib/tfutil.py:74: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/stylegan/dnnlib/tflib/tfutil.py:128: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Le8L_E6_yad_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8be4ab6-5df5-4c3e-f0d2-5350e45a1e05"
      },
      "source": [
        "# Loads the model StyleGAN FFHQ 1024x1024 from personal google drive link\n",
        "\n",
        "# Initialize TensorFlow.\n",
        "tflib.init_tf()\n",
        "# Load pre-trained network.\n",
        "url = 'https://drive.google.com/uc?id=1_zKCCV3R7KSDLlgQpz1PbMGTUy4rY97v' # karras2019stylegan-ffhq-1024x1024.pkl\n",
        "with dnnlib.util.open_url(url, cache_dir=config.cache_dir) as f:\n",
        "    _G, _D, Gs = pickle.load(f)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/stylegan/dnnlib/tflib/tfutil.py:97: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/stylegan/dnnlib/tflib/tfutil.py:109: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/stylegan/dnnlib/tflib/tfutil.py:132: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "Downloading https://drive.google.com/uc?id=1_zKCCV3R7KSDLlgQpz1PbMGTUy4rY97v .... done\n",
            "WARNING:tensorflow:From /content/stylegan/dnnlib/tflib/network.py:142: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/stylegan/dnnlib/tflib/network.py:150: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/stylegan/dnnlib/tflib/tfutil.py:76: The name tf.VariableScope is deprecated. Please use tf.compat.v1.VariableScope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/stylegan/dnnlib/tflib/network.py:151: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/stylegan/dnnlib/tflib/network.py:154: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/stylegan/dnnlib/tflib/network.py:182: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/stylegan/dnnlib/tflib/tfutil.py:200: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From <string>:364: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOmse5ow8Q9O"
      },
      "source": [
        "# Pick latent vector.\n",
        "rnd = np.random.RandomState(1000)\n",
        "total_imgs = 100\n",
        "\n",
        "all_latents = np.empty((total_imgs, Gs.input_shape[1])) # save all images \n",
        "# OOM error if we try 100 images at the same time\n",
        "for j in range(total_imgs//50):\n",
        "    latents = rnd.randn(50, Gs.input_shape[1])\n",
        "    all_latents[j*50:j*50+50] = latents.copy()\n",
        "\n",
        "    # Generate image.\n",
        "    fmt = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n",
        "    images = Gs.run(latents, None, truncation_psi=0.7, randomize_noise=True, output_transform=fmt)\n",
        "\n",
        "    # Save image.\n",
        "    os.makedirs(config.result_dir, exist_ok=True)\n",
        "    for i, image in enumerate(images):\n",
        "        png_filename = os.path.join(config.result_dir, f'example_{i+50*j}.png')\n",
        "        PIL.Image.fromarray(images[i], 'RGB').save(png_filename)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoytF8h32PJo"
      },
      "source": [
        "# Saving all images in the latent space W\n",
        "all_src_dlatents = np.empty((total_imgs, 18, Gs.input_shape[1]))\n",
        "for j in range(total_imgs//50):\n",
        "    src_dlatents = Gs.components.mapping.run(all_latents[j*50:j*50+50], None)\n",
        "    all_src_dlatents[j*50:j*50+50] = src_dlatents.copy()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkjQogO8606e"
      },
      "source": [
        "# Block to do the weighted average between two images\n",
        "\n",
        "weights = np.linspace(0, 1, 10)\n",
        "# We chose manually the images at index 0 and index 11, as we can see below\n",
        "latent_mixed = [weights[i]*all_latents[0] + (1-weights[i])*all_latents[11] for i in range(len(weights))]\n",
        "latent_mixed = np.array(latent_mixed)\n",
        "img_mixed = Gs.run(latent_mixed, None, truncation_psi=0.7, randomize_noise=True, output_transform=fmt)\n",
        "\n",
        "# Saving images in the folder ./stylegan/linvar\n",
        "os.makedirs('./linvar', exist_ok=True)\n",
        "for i, image in enumerate(img_mixed):\n",
        "    png_filename = os.path.join('./linvar', f'{round(100*weights[i])} {round(100-100*weights[i])}.png')\n",
        "    PIL.Image.fromarray(img_mixed[i], 'RGB').save(png_filename)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRIm6roWTAfW"
      },
      "source": [
        "# Changing manually a coordinate in the latent space W to get an artifact\n",
        "i = 46\n",
        "test_image = all_latents[0].copy()\n",
        "test_image[46] = 20#-np.cos(i)*2*i\n",
        "test_image = np.array([test_image])\n",
        "!mkdir ./tests\n",
        "png_filename = os.path.join('./tests/', f'test{i}.png')\n",
        "PIL.Image.fromarray(Gs.run(test_image, None, truncation_psi=0.7, randomize_noise=True, output_transform=fmt)[0], 'RGB').save(png_filename)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzAmcdK8EoAv"
      },
      "source": [
        "# Saving others images to use in the InterfaceGAN\n",
        "# Tried to chose images with a good variability of characteristics\n",
        "\n",
        "os.makedirs('./latents', exist_ok=True)\n",
        "imgs_chosen = [0, 1, 8, 10, 11, 13, 14, 32, 55, 58, 68, 95, 97]\n",
        "np.save(f'./latents/seed1000_imgs.npy', all_src_dlatents[imgs_chosen][:, 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCI9haUYOO9N",
        "outputId": "084e50d9-d647-4722-9aad-d884981db532"
      },
      "source": [
        "# We go back to root dir\n",
        "os.chdir(\"..\")\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data  stylegan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6fzMfBWLH-7"
      },
      "source": [
        "## InterfaceGAN\n",
        "\n",
        "We always ran out of memory in this part if we didn't reset the environment once or more times. To use this, after saving the images to be used in InterfaceGAN one must reset the environment until we don't run OOM when using InterfaceGAN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ik5g4kcN9v2",
        "outputId": "f87e2104-ecb2-418c-fe14-38cdadbe9484"
      },
      "source": [
        "! git clone https://github.com/genforce/interfacegan"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'interfacegan'...\n",
            "remote: Enumerating objects: 252, done.\u001b[K\n",
            "remote: Counting objects:  20% (1/5)\u001b[K\rremote: Counting objects:  40% (2/5)\u001b[K\rremote: Counting objects:  60% (3/5)\u001b[K\rremote: Counting objects:  80% (4/5)\u001b[K\rremote: Counting objects: 100% (5/5)\u001b[K\rremote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 252 (delta 0), reused 0 (delta 0), pack-reused 247\u001b[K\n",
            "Receiving objects: 100% (252/252), 11.39 MiB | 40.37 MiB/s, done.\n",
            "Resolving deltas: 100% (85/85), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUFqp48bEtgb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8182b0d5-61c3-4aa1-b6e2-b6f27bb9a0d3"
      },
      "source": [
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "\n",
        "gdd.download_file_from_google_drive(file_id='1_zKCCV3R7KSDLlgQpz1PbMGTUy4rY97v',\n",
        "                                    dest_path='./interfacegan/models/pretrain/karras2019stylegan-ffhq-1024x1024.pkl',\n",
        "                                    unzip=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 1_zKCCV3R7KSDLlgQpz1PbMGTUy4rY97v into ./interfacegan/models/pretrain/karras2019stylegan-ffhq-1024x1024.pkl... Done.\n",
            "Unzipping..."
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/google_drive_downloader/google_drive_downloader.py:78: UserWarning: Ignoring `unzip` since \"1_zKCCV3R7KSDLlgQpz1PbMGTUy4rY97v\" does not look like a valid zip file\n",
            "  warnings.warn('Ignoring `unzip` since \"{}\" does not look like a valid zip file'.format(file_id))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqzAGS30O5wV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7764c722-28cb-46f0-e95b-3b67dc61cf23"
      },
      "source": [
        "# If you run two cell aboves and fail, you must delete all folder created to run again\n",
        "# Which is pretty sad when we need to restart the environment multiple times\n",
        "\n",
        "!rm -r ./interfacegan/results/stylegan_ffhq_age\n",
        "!rm -r ./interfacegan/results/stylegan_ffhq_pose\n",
        "!rm -r ./interfacegan/results/stylegan_ffhq_smile\n",
        "!rm -r ./interfacegan/results/stylegan_ffhq_gender\n",
        "!rm -r ./interfacegan/results/stylegan_ffhq_eyeglasses"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove './interfacegan/results/stylegan_ffhq_age': No such file or directory\n",
            "rm: cannot remove './interfacegan/results/stylegan_ffhq_smile': No such file or directory\n",
            "rm: cannot remove './interfacegan/results/stylegan_ffhq_gender': No such file or directory\n",
            "rm: cannot remove './interfacegan/results/stylegan_ffhq_eyeglasses': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WbsBTJzPvbU",
        "outputId": "bf9cc76e-869a-43b4-8572-00787829b5c2"
      },
      "source": [
        "# Command line to run interfacegan in the age boundary\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "\n",
        "!python ./interfacegan/edit.py -s W \\\n",
        "    -m stylegan_ffhq \\\n",
        "    -b ./interfacegan/boundaries/stylegan_ffhq_age_w_boundary.npy \\\n",
        "    -i \"./stylegan/latents/seed1000_imgs.npy\" \\\n",
        "    --start_distance -4 \\\n",
        "    --end_distance 4 \\\n",
        "    -o ./interfacegan/results/stylegan_ffhq_age \\\n",
        "    --steps 9"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2021-06-21 09:39:10,899][INFO] Initializing generator.\n",
            "[2021-06-21 09:39:11,126][INFO] Loading pytorch model from `interfacegan/models/pretrain/stylegan_ffhq.pth`.\n",
            "[2021-06-21 09:39:11,354][INFO] Successfully loaded!\n",
            "[2021-06-21 09:39:11,355][INFO]   `lod` of the loaded model is 0.0.\n",
            "[2021-06-21 09:39:14,658][INFO] Preparing boundary.\n",
            "[2021-06-21 09:39:14,662][INFO] Preparing latent codes.\n",
            "[2021-06-21 09:39:14,662][INFO]   Load latent codes from `./stylegan/latents/seed1000_imgs.npy`.\n",
            "[2021-06-21 09:39:14,662][INFO] Editing 13 samples.\n",
            "[2021-06-21 09:39:24,218][INFO] Successfully edited 13 samples.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsZjjHup6oKV",
        "outputId": "1fe26174-3fed-43e2-9f8a-bd73996e926b"
      },
      "source": [
        "# Pose boundary, ..., etc\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "\n",
        "!python ./interfacegan/edit.py -s W \\\n",
        "    -m stylegan_ffhq \\\n",
        "    -b ./interfacegan/boundaries/stylegan_ffhq_pose_w_boundary.npy \\\n",
        "    -i \"./stylegan/latents/seed1000_imgs.npy\" \\\n",
        "    --start_distance -4 \\\n",
        "    --end_distance 4 \\\n",
        "    -o ./interfacegan/results/stylegan_ffhq_pose \\\n",
        "    --steps 9"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2021-06-21 09:39:26,524][INFO] Initializing generator.\n",
            "[2021-06-21 09:39:26,747][INFO] Loading pytorch model from `interfacegan/models/pretrain/stylegan_ffhq.pth`.\n",
            "[2021-06-21 09:39:26,999][INFO] Successfully loaded!\n",
            "[2021-06-21 09:39:27,000][INFO]   `lod` of the loaded model is 0.0.\n",
            "[2021-06-21 09:39:30,266][INFO] Preparing boundary.\n",
            "[2021-06-21 09:39:30,270][INFO] Preparing latent codes.\n",
            "[2021-06-21 09:39:30,270][INFO]   Load latent codes from `./stylegan/latents/seed1000_imgs.npy`.\n",
            "[2021-06-21 09:39:30,271][INFO] Editing 13 samples.\n",
            "[2021-06-21 09:39:39,861][INFO] Successfully edited 13 samples.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXc6inFG6zws",
        "outputId": "d9e2bbd9-ebf6-4d43-bb02-6265f5e15967"
      },
      "source": [
        "# Smile\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "\n",
        "!python ./interfacegan/edit.py -s W \\\n",
        "    -m stylegan_ffhq \\\n",
        "    -b ./interfacegan/boundaries/stylegan_ffhq_smile_w_boundary.npy \\\n",
        "    -i \"./stylegan/latents/seed1000_imgs.npy\" \\\n",
        "    --start_distance -4 \\\n",
        "    --end_distance 4 \\\n",
        "    -o ./interfacegan/results/stylegan_ffhq_smile \\\n",
        "    --steps 9"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2021-06-21 09:39:41,417][INFO] Initializing generator.\n",
            "[2021-06-21 09:39:41,635][INFO] Loading pytorch model from `interfacegan/models/pretrain/stylegan_ffhq.pth`.\n",
            "[2021-06-21 09:39:41,856][INFO] Successfully loaded!\n",
            "[2021-06-21 09:39:41,857][INFO]   `lod` of the loaded model is 0.0.\n",
            "[2021-06-21 09:39:45,119][INFO] Preparing boundary.\n",
            "[2021-06-21 09:39:45,123][INFO] Preparing latent codes.\n",
            "[2021-06-21 09:39:45,123][INFO]   Load latent codes from `./stylegan/latents/seed1000_imgs.npy`.\n",
            "[2021-06-21 09:39:45,123][INFO] Editing 13 samples.\n",
            "[2021-06-21 09:39:54,697][INFO] Successfully edited 13 samples.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrPmYYkg6zzm",
        "outputId": "18376f0a-06cb-4d50-a46a-9b566f0b2fb8"
      },
      "source": [
        "# Gender\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "\n",
        "!python ./interfacegan/edit.py -s W \\\n",
        "    -m stylegan_ffhq \\\n",
        "    -b ./interfacegan/boundaries/stylegan_ffhq_gender_w_boundary.npy \\\n",
        "    -i \"./stylegan/latents/seed1000_imgs.npy\" \\\n",
        "    --start_distance -4 \\\n",
        "    --end_distance 4 \\\n",
        "    -o ./interfacegan/results/stylegan_ffhq_gender \\\n",
        "    --steps 9"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2021-06-21 09:39:56,211][INFO] Initializing generator.\n",
            "[2021-06-21 09:39:56,435][INFO] Loading pytorch model from `interfacegan/models/pretrain/stylegan_ffhq.pth`.\n",
            "[2021-06-21 09:39:56,682][INFO] Successfully loaded!\n",
            "[2021-06-21 09:39:56,682][INFO]   `lod` of the loaded model is 0.0.\n",
            "[2021-06-21 09:39:59,954][INFO] Preparing boundary.\n",
            "[2021-06-21 09:39:59,958][INFO] Preparing latent codes.\n",
            "[2021-06-21 09:39:59,958][INFO]   Load latent codes from `./stylegan/latents/seed1000_imgs.npy`.\n",
            "[2021-06-21 09:39:59,959][INFO] Editing 13 samples.\n",
            "[2021-06-21 09:40:09,573][INFO] Successfully edited 13 samples.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMd6yaki6z-T",
        "outputId": "4ac0f4bf-a466-410e-b515-d2d73f1c39b8"
      },
      "source": [
        "# Eyeglasses\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "\n",
        "!python ./interfacegan/edit.py -s W \\\n",
        "    -m stylegan_ffhq \\\n",
        "    -b ./interfacegan/boundaries/stylegan_ffhq_eyeglasses_w_boundary.npy \\\n",
        "    -i \"./stylegan/latents/seed1000_imgs.npy\" \\\n",
        "    --start_distance -4 \\\n",
        "    --end_distance 4 \\\n",
        "    -o ./interfacegan/results/stylegan_ffhq_eyeglasses \\\n",
        "    --steps 9"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2021-06-21 09:40:11,148][INFO] Initializing generator.\n",
            "[2021-06-21 09:40:11,368][INFO] Loading pytorch model from `interfacegan/models/pretrain/stylegan_ffhq.pth`.\n",
            "[2021-06-21 09:40:11,604][INFO] Successfully loaded!\n",
            "[2021-06-21 09:40:11,605][INFO]   `lod` of the loaded model is 0.0.\n",
            "[2021-06-21 09:40:14,871][INFO] Preparing boundary.\n",
            "[2021-06-21 09:40:14,875][INFO] Preparing latent codes.\n",
            "[2021-06-21 09:40:14,875][INFO]   Load latent codes from `./stylegan/latents/seed1000_imgs.npy`.\n",
            "[2021-06-21 09:40:14,876][INFO] Editing 13 samples.\n",
            "[2021-06-21 09:40:24,492][INFO] Successfully edited 13 samples.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7K-Iwa8o4-S"
      },
      "source": [
        "# Zipping and downloading results\n",
        "\n",
        "!zip -r ./results.zip ./interfacegan/results\n",
        "from google.colab import files\n",
        "files.download(\"./results.zip\")\n",
        "\n",
        "!zip -r ./linvar.zip ./linvar\n",
        "from google.colab import files\n",
        "files.download(\"./linvar.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFTI5j9RMQ25"
      },
      "source": [
        "## Image inversion (from real image to latent space W)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xiw21cyDNJHh"
      },
      "source": [
        "The blocks of code below are almost the same as found in the repository https://github.com/genforce/idinvert_pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Te4zumWMfGO"
      },
      "source": [
        "import os\n",
        "\n",
        "os.chdir('/content')\n",
        "CODE_DIR = 'idinvert'\n",
        "if not os.path.exists(CODE_DIR):\n",
        "  !git clone https://github.com/genforce/idinvert_pytorch.git $CODE_DIR\n",
        "os.chdir(f'./{CODE_DIR}')\n",
        "MODEL_DIR = os.path.join('models', 'pretrain')\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "!wget https://mycuhk-my.sharepoint.com/:u:/g/personal/1155082926_link_cuhk_edu_hk/EXqix_JIEgtLl1FXI4uCkr8B5GPaiJyiLXL6cFbdcIKqEA?e=WYesel\\&download\\=1 -O $MODEL_DIR/styleganinv_ffhq256_encoder.pth  --quiet\n",
        "!wget https://mycuhk-my.sharepoint.com/:u:/g/personal/1155082926_link_cuhk_edu_hk/EbuzMQ3ZLl1AqvKJzeeBq7IBoQD-C1LfMIC8USlmOMPt3Q?e=CMXn8W\\&download\\=1 -O $MODEL_DIR/styleganinv_ffhq256_generator.pth  --quiet\n",
        "!wget https://mycuhk-my.sharepoint.com/:u:/g/personal/1155082926_link_cuhk_edu_hk/EQJUz9DInbxEnp0aomkGGzAB5b3ZZbtsOA-TXct9E4ONqA?e=smtO0T\\&download\\=1 -O $MODEL_DIR/vgg16.pth  --quiet\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvfuUAQgMgBu"
      },
      "source": [
        "# python 3.6\n",
        "\"\"\"Demo.\"\"\"\n",
        "import os\n",
        "import sys\n",
        "import io\n",
        "import bz2\n",
        "import requests\n",
        "import dlib\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import IPython.display\n",
        "import scipy.ndimage\n",
        "from google.colab import files\n",
        "from google.colab import output\n",
        "from utils.editor import manipulate\n",
        "from utils.inverter import StyleGANInverter\n",
        "from models.helper import build_generator\n",
        "\n",
        "\n",
        "LANDMARK_MODEL_NAME = 'shape_predictor_68_face_landmarks.dat'\n",
        "LANDMARK_MODEL_PATH = os.path.join(MODEL_DIR, LANDMARK_MODEL_NAME)\n",
        "LANDMARK_MODEL_URL = f'http://dlib.net/files/{LANDMARK_MODEL_NAME}.bz2'\n",
        "model_name = 'styleganinv_ffhq256'\n",
        "pre = 'examples'\n",
        "inverted_code_dir = 'inverted_codes'\n",
        "os.makedirs(inverted_code_dir, exist_ok=True)\n",
        "\n",
        "class FaceLandmarkDetector(object):\n",
        "  \"\"\"Class of face landmark detector.\"\"\"\n",
        "\n",
        "  def _init_(self, align_size=256, enable_padding=True):\n",
        "    \"\"\"Initializes face detector and landmark detector.\n",
        "\n",
        "  Args:\n",
        "    align_size: Size of the aligned face if performing face alignment.\n",
        "    (default: 1024)\n",
        "    enable_padding: Whether to enable padding for face alignment (default:\n",
        "    True)\n",
        "  \"\"\"\n",
        "    # Download models if needed.\n",
        "    if not os.path.exists(LANDMARK_MODEL_PATH):\n",
        "      data = requests.get(LANDMARK_MODEL_URL)\n",
        "      data_decompressed = bz2.decompress(data.content)\n",
        "      with open(LANDMARK_MODEL_PATH, 'wb') as f:\n",
        "        f.write(data_decompressed)\n",
        "\n",
        "    self.face_detector = dlib.get_frontal_face_detector()\n",
        "    self.landmark_detector = dlib.shape_predictor(LANDMARK_MODEL_PATH)\n",
        "    self.align_size = align_size\n",
        "    self.enable_padding = enable_padding\n",
        "\n",
        "  def detect(self, image_path):\n",
        "    \"\"\"Detects landmarks from the given image.\n",
        "\n",
        "  This function will first perform face detection on the input image. All\n",
        "  detected results will be grouped into a list. If no face is detected, an\n",
        "  empty list will be returned.\n",
        "\n",
        "  For each element in the list, it is a dictionary consisting of `image_path`,\n",
        "  `bbox` and `landmarks`. `image_path` is the path to the input image. `bbox`\n",
        "  is the 4-element bounding box with order (left, top, right, bottom), and\n",
        "  `landmarks` is a list of 68 (x, y) points.\n",
        "\n",
        "  Args:\n",
        "    image_path: Path to the image to detect landmarks from.\n",
        "\n",
        "  Returns:\n",
        "    A list of dictionaries, each of which is the detection results of a\n",
        "    particular face.\n",
        "  \"\"\"\n",
        "    results = []\n",
        "\n",
        "    # image_ = np.array(image)\n",
        "    images = dlib.load_rgb_image(image_path)\n",
        "    # Face detection (1 means to upsample the image for 1 time.)\n",
        "    bboxes = self.face_detector(images, 1)\n",
        "    # Landmark detection\n",
        "    for bbox in bboxes:\n",
        "      landmarks = []\n",
        "      for point in self.landmark_detector(images, bbox).parts():\n",
        "        landmarks.append((point.x, point.y))\n",
        "      results.append({\n",
        "          'image_path': image_path,\n",
        "          'bbox': (bbox.left(), bbox.top(), bbox.right(), bbox.bottom()),\n",
        "          'landmarks': landmarks,\n",
        "      })\n",
        "    return results\n",
        "\n",
        "  def align(self, face_info):\n",
        "    \"\"\"Aligns face based on landmark detection.\n",
        "\n",
        "  The face alignment process is borrowed from\n",
        "  https://github.com/NVlabs/ffhq-dataset/blob/master/download_ffhq.py,\n",
        "  which only supports aligning faces to square size.\n",
        "\n",
        "  Args:\n",
        "    face_info: Face information, which is the element of the list returned by\n",
        "    `self.detect()`.\n",
        "\n",
        "  Returns:\n",
        "    A `np.ndarray`, containing the aligned result. It is with `RGB` channel\n",
        "    order.\n",
        "  \"\"\"\n",
        "    img = Image.open(face_info['image_path'])\n",
        "\n",
        "    landmarks = np.array(face_info['landmarks'])\n",
        "    eye_left = np.mean(landmarks[36: 42], axis=0)\n",
        "    eye_right = np.mean(landmarks[42: 48], axis=0)\n",
        "    eye_middle = (eye_left + eye_right) / 2\n",
        "    eye_to_eye = eye_right - eye_left\n",
        "    mouth_middle = (landmarks[48] + landmarks[54]) / 2\n",
        "    eye_to_mouth = mouth_middle - eye_middle\n",
        "\n",
        "    # Choose oriented crop rectangle.\n",
        "    x = eye_to_eye - np.flipud(eye_to_mouth) * [-1, 1]\n",
        "    x /= np.hypot(*x)\n",
        "    x *= max(np.hypot(*eye_to_eye) * 2.0, np.hypot(*eye_to_mouth) * 1.8)\n",
        "    y = np.flipud(x) * [-1, 1]\n",
        "    c = eye_middle + eye_to_mouth * 0.1\n",
        "    quad = np.stack([c - x - y, c - x + y, c + x + y, c + x - y])\n",
        "    qsize = np.hypot(*x) * 2\n",
        "\n",
        "    # Shrink.\n",
        "    shrink = int(np.floor(qsize / self.align_size * 0.5))\n",
        "    if shrink > 1:\n",
        "      rsize = (int(np.rint(float(img.size[0]) / shrink)),\n",
        "               int(np.rint(float(img.size[1]) / shrink)))\n",
        "      img = img.resize(rsize, Image.ANTIALIAS)\n",
        "      quad /= shrink\n",
        "      qsize /= shrink\n",
        "\n",
        "    # Crop.\n",
        "    border = max(int(np.rint(qsize * 0.1)), 3)\n",
        "    crop = (int(np.floor(min(quad[:, 0]))), int(np.floor(min(quad[:, 1]))),\n",
        "            int(np.ceil(max(quad[:, 0]))), int(np.ceil(max(quad[:, 1]))))\n",
        "    crop = (max(crop[0] - border, 0),\n",
        "            max(crop[1] - border, 0),\n",
        "            min(crop[2] + border, img.size[0]),\n",
        "            min(crop[3] + border, img.size[1]))\n",
        "    if crop[2] - crop[0] < img.size[0] or crop[3] - crop[1] < img.size[1]:\n",
        "      img = img.crop(crop)\n",
        "      quad -= crop[0:2]\n",
        "\n",
        "    # Pad.\n",
        "    pad = (int(np.floor(min(quad[:, 0]))), int(np.floor(min(quad[:, 1]))),\n",
        "           int(np.ceil(max(quad[:, 0]))), int(np.ceil(max(quad[:, 1]))))\n",
        "    pad = (max(-pad[0] + border, 0),\n",
        "           max(-pad[1] + border, 0),\n",
        "           max(pad[2] - img.size[0] + border, 0),\n",
        "           max(pad[3] - img.size[1] + border, 0))\n",
        "    if self.enable_padding and max(pad) > border - 4:\n",
        "      pad = np.maximum(pad, int(np.rint(qsize * 0.3)))\n",
        "      img = np.pad(np.float32(img),\n",
        "                   ((pad[1], pad[3]), (pad[0], pad[2]), (0, 0)),\n",
        "                   'reflect')\n",
        "      h, w, _ = img.shape\n",
        "      y, x, _ = np.ogrid[:h, :w, :1]\n",
        "      mask = np.maximum(1.0 - np.minimum(np.float32(x) / pad[0],\n",
        "                                         np.float32(w - 1 - x) / pad[2]),\n",
        "                        1.0 - np.minimum(np.float32(y) / pad[1],\n",
        "                                         np.float32(h - 1 - y) / pad[3]))\n",
        "      blur = qsize * 0.02\n",
        "      blurred_image = scipy.ndimage.gaussian_filter(img, [blur, blur, 0]) - img\n",
        "      img += blurred_image * np.clip(mask * 3.0 + 1.0, 0.0, 1.0)\n",
        "      img += (np.median(img, axis=(0, 1)) - img) * np.clip(mask, 0.0, 1.0)\n",
        "      img = Image.fromarray(np.uint8(np.clip(np.rint(img), 0, 255)), 'RGB')\n",
        "      quad += pad[:2]\n",
        "\n",
        "    # Transform.\n",
        "    img = img.transform((self.align_size * 4, self.align_size * 4), Image.QUAD,\n",
        "                        (quad + 0.5).flatten(), Image.BILINEAR)\n",
        "    img = img.resize((self.align_size, self.align_size), Image.ANTIALIAS)\n",
        "\n",
        "    return np.array(img)\n",
        "\n",
        "\n",
        "def align_face(image_path, align_size=256):\n",
        "  \"\"\"Aligns a given face.\"\"\"\n",
        "  model = FaceLandmarkDetector(align_size)\n",
        "  face_infos = model.detect(image_path)\n",
        "  face_infos = face_infos[0]\n",
        "  img = model.align(face_infos)\n",
        "  return img\n",
        "\n",
        "\n",
        "def build_inverter(model_name, iteration=100, regularization_loss_weight=2):\n",
        "  \"\"\"Builds inverter\"\"\"\n",
        "  inverter = StyleGANInverter(\n",
        "      model_name,\n",
        "      learning_rate=0.01,\n",
        "      iteration=iteration,\n",
        "      reconstruction_loss_weight=1.0,\n",
        "      perceptual_loss_weight=5e-5,\n",
        "      regularization_loss_weight=regularization_loss_weight)\n",
        "  return inverter\n",
        "\n",
        "\n",
        "def get_generator(model_name):\n",
        "  \"\"\"Gets model by name\"\"\"\n",
        "  return build_generator(model_name)\n",
        "\n",
        "\n",
        "def align(inverter, image_path):\n",
        "  \"\"\"Aligns an unloaded image.\"\"\"\n",
        "  aligned_image = align_face(image_path,\n",
        "                             align_size=inverter.G.resolution)\n",
        "  return aligned_image\n",
        "\n",
        "\n",
        "def invert(inverter, image):\n",
        "  \"\"\"Inverts an image.\"\"\"\n",
        "  latent_code, reconstruction = inverter.easy_invert(image, num_viz=1)\n",
        "  return latent_code, reconstruction\n",
        "\n",
        "\n",
        "def diffuse(inverter, target, context, left, top, width, height):\n",
        "  \"\"\"Diffuses a target image to a context image.\"\"\"\n",
        "  center_x = left + width // 2\n",
        "  center_y = top + height // 2\n",
        "  _, diffusion = inverter.easy_diffuse(target=target,\n",
        "                                       context=context,\n",
        "                                       center_x=center_x,\n",
        "                                       center_y=center_y,\n",
        "                                       crop_x=width,\n",
        "                                       crop_y=height,\n",
        "                                       num_viz=1)\n",
        "  return diffusion\n",
        "\n",
        "\n",
        "def load_image(path):\n",
        "  \"\"\"Loads an image from disk.\n",
        "\n",
        "  NOTE: This function will always return an image with `RGB` channel order for\n",
        "  color image and pixel range [0, 255].\n",
        "\n",
        "  Args:\n",
        "    path: Path to load the image from.\n",
        "\n",
        "  Returns:\n",
        "    An image with dtype `np.ndarray` or `None` if input `path` does not exist.\n",
        "  \"\"\"\n",
        "  if not os.path.isfile(path):\n",
        "    return None\n",
        "\n",
        "  image = Image.open(path)\n",
        "  return image\n",
        "\n",
        "def imshow(images, col, viz_size=256):\n",
        "  \"\"\"Shows images in one figure.\"\"\"\n",
        "  num, height, width, channels = images.shape\n",
        "  assert num % col == 0\n",
        "  row = num // col\n",
        "\n",
        "  fused_image = np.zeros((viz_size * row, viz_size * col, channels), dtype=np.uint8)\n",
        "\n",
        "  for idx, image in enumerate(images):\n",
        "    i, j = divmod(idx, col)\n",
        "    y = i * viz_size\n",
        "    x = j * viz_size\n",
        "    if height != viz_size or width != viz_size:\n",
        "      image = cv2.resize(image, (viz_size, viz_size))\n",
        "    fused_image[y:y + viz_size, x:x + viz_size] = image\n",
        "\n",
        "  fused_image = np.asarray(fused_image, dtype=np.uint8)\n",
        "  data = io.BytesIO()\n",
        "  if channels == 4:\n",
        "    Image.fromarray(fused_image).save(data, 'png')\n",
        "  elif channels == 3:\n",
        "    Image.fromarray(fused_image).save(data, 'jpeg')\n",
        "  else:\n",
        "    raise ValueError('Image channel error')\n",
        "  im_data = data.getvalue()\n",
        "  disp = IPython.display.display(IPython.display.Image(im_data))\n",
        "  return disp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7N5xIk5NYE3"
      },
      "source": [
        "print('Building inverter')\n",
        "inverter = build_inverter(model_name=model_name)\n",
        "print('Building generator')\n",
        "generator = get_generator(model_name)\n",
        "output.clear()\n",
        "print('Please upload the image you want to manipulate or \\\n",
        "use the default images by clicking `Cancel upload` button.')\n",
        "uploaded = files.upload()\n",
        "if uploaded:\n",
        "  image_name = list(uploaded.keys())[0]\n",
        "  mani_image = align(inverter, image_name)\n",
        "  if mani_image.shape[2] == 4:\n",
        "    mani_image = mani_image[:, :, :3]\n",
        "  os.remove(image_name)\n",
        "else:\n",
        "  image_name = '000006.png'\n",
        "  im_name = os.path.join(pre, image_name)\n",
        "  mani_image = align(inverter, im_name)\n",
        "print('Image ready, starting inversion!!!')\n",
        "sys.stdout.flush()\n",
        "\n",
        "latent_code_path = os.path.join(inverted_code_dir, \n",
        "                                image_name.split('.')[0] + '.npy')\n",
        "if not os.path.exists(latent_code_path):\n",
        "  latent_code, _ = invert(inverter, mani_image)\n",
        "  np.save(latent_code_path, latent_code)\n",
        "else:\n",
        "  print('code already exists, skip inversion!!!')\n",
        "  latent_code = np.load(latent_code_path)\n",
        "\n",
        "ATTRS = ['age', 'eyeglasses', 'gender', 'pose', 'expression']\n",
        "boundaries = {}\n",
        "for attr in ATTRS:\n",
        "  boundary_path = os.path.join('./boundaries', \n",
        "                               'stylegan_ffhq256', attr + '.npy')\n",
        "  boundary_file = np.load(boundary_path, allow_pickle=True)[()]\n",
        "  boundary = boundary_file['boundary']\n",
        "  manipulate_layers = boundary_file['meta_data']['manipulate_layers']\n",
        "  boundaries[attr] = []\n",
        "  boundaries[attr].append(boundary)\n",
        "  boundaries[attr].append(manipulate_layers)\n",
        "print()\n",
        "print('Image inversion completed, please use the next block to manipulate!!!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XcHg0dHNaE_"
      },
      "source": [
        "#@title { display-mode: \"form\", run: \"auto\" }\n",
        "\n",
        "age = 0 #@param {type:\"slider\", min:-3.0, max:3.0, step:0.1}\n",
        "eyeglasses = 0 #@param {type:\"slider\", min:-2.9, max:3.0, step:0.1}\n",
        "gender = -2.4 #@param {type:\"slider\", min:-3.0, max:3.0, step:0.1}\n",
        "pose = 0 #@param {type:\"slider\", min:-3.0, max:3.0, step:0.1}\n",
        "expression = 0 #@param {type:\"slider\", min:-3.0, max:3.0, step:0.1}\n",
        "\n",
        "\n",
        "new_codes = latent_code.copy()\n",
        "for i, attr_name in enumerate(ATTRS):\n",
        "  manipulate_layers = boundaries[attr_name][1]\n",
        "  new_codes[:, manipulate_layers, :] += boundaries[attr_name][0][:, manipulate_layers, :] * eval(attr_name)\n",
        "\n",
        "new_images = generator.easy_synthesize(new_codes, **{'latent_space_type': 'wp'})['image']\n",
        "showed_images = np.concatenate([mani_image[np.newaxis], new_images], axis=0)\n",
        "imshow(showed_images, col=showed_images.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GT41SbaNgoj"
      },
      "source": [
        "### Interpolation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbgZfnzdNaHx"
      },
      "source": [
        "print('Building inverter')\n",
        "inverter = build_inverter(model_name=model_name)\n",
        "print('Building generator')\n",
        "generator = get_generator(model_name)\n",
        "output.clear()\n",
        "def linear_interpolate(src_code, dst_code, step=5):\n",
        "  \"\"\"Interpolates two latent codes linearlly.\n",
        "  Args:\n",
        "    src_code: Source code, with shape [1, latent_space_dim].\n",
        "    dst_code: Target code, with shape [1, latent_space_dim].\n",
        "    step: Number of interploation steps. (default: 5)\n",
        "  Returns:\n",
        "    Interpolated code, with shape [step, latent_space_dim].\n",
        "  \"\"\"\n",
        "  assert (len(src_code.shape) == 2 and len(dst_code.shape) == 2 and\n",
        "          src_code.shape[0] == 1 and dst_code.shape[0] == 1 and\n",
        "          src_code.shape[1] == dst_code.shape[1])\n",
        "\n",
        "  linspace = np.linspace(0.0, 1.0, step)[:, np.newaxis].astype(np.float32)\n",
        "  return src_code + linspace * (dst_code - src_code)\n",
        "\n",
        "print('Please upload the source image or \\\n",
        "use the default image by clicking `Cancel upload` button.')\n",
        "uploaded = files.upload()\n",
        "if uploaded:\n",
        "  src_image_name = list(uploaded.keys())[0]\n",
        "  src_image = align(inverter, src_image_name)\n",
        "  if src_image.shape[2] == 4:\n",
        "    src_image = src_image[:, :, :3]\n",
        "  os.remove(src_image_name)\n",
        "else:\n",
        "  src_image_name = '000008.png'\n",
        "  im_name = os.path.join(pre, src_image_name)\n",
        "  src_image = align(inverter, im_name)\n",
        "print('Source image ready!!!')\n",
        "src_code_path = os.path.join(inverted_code_dir, \n",
        "                             src_image_name.split('.')[0] + '.npy')\n",
        "\n",
        "print('Please upload the target image or \\\n",
        "use the default image by clicking `Cancel upload` button.')\n",
        "uploaded = files.upload()\n",
        "if uploaded:\n",
        "  dst_image_name = list(uploaded.keys())[0]\n",
        "  dst_image = align(inverter, dst_image_name)\n",
        "  if dst_image.shape[2] == 4:\n",
        "    dst_image = dst_image[:, :, :3]\n",
        "  os.remove(dst_image_name)\n",
        "else:\n",
        "  dst_image_name = '000013.png'\n",
        "  im_name = os.path.join(pre, dst_image_name)\n",
        "  dst_image = align(inverter, im_name)\n",
        "print('Target image ready!!!')\n",
        "sys.stdout.flush()\n",
        "dst_code_path = os.path.join(inverted_code_dir, \n",
        "                             dst_image_name.split('.')[0] + '.npy')\n",
        "\n",
        "if not os.path.exists(src_code_path):\n",
        "  src_code, _ = invert(inverter, src_image)\n",
        "  np.save(src_code_path, src_code)\n",
        "else:\n",
        "  src_code = np.load(src_code_path)\n",
        "\n",
        "\n",
        "if not os.path.exists(dst_code_path):\n",
        "  dst_code, _ = invert(inverter, dst_image)\n",
        "  np.save(dst_code_path, dst_code)\n",
        "else:\n",
        "  dst_code = np.load(dst_code_path)\n",
        "print()\n",
        "print('Both the source image and target image are inverted, \\\n",
        "please use the next block to interpolate!!!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9893dHnNaJl"
      },
      "source": [
        "#@title { display-mode: \"form\", run: \"auto\" }\n",
        "step = 2 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "\n",
        "inter_images = []\n",
        "inter_images.insert(0, dst_image)\n",
        "inter_images.insert(-1, src_image)\n",
        "\n",
        "inter_codes = linear_interpolate(np.reshape(src_code, [1, -1]),\n",
        "                                 np.reshape(dst_code, [1, -1]),\n",
        "                                 step=step)\n",
        "inter_codes = np.reshape(inter_codes, [-1, inverter.G.num_layers, inverter.G.w_space_dim])\n",
        "inter_imgs = generator.easy_synthesize(inter_codes, **{'latent_space_type': 'wp'})['image']\n",
        "\n",
        "for ind in range(inter_imgs.shape[0]):\n",
        "  inter_images.insert(ind+1, inter_imgs[ind])\n",
        "\n",
        "inter_images = np.asarray(inter_images)\n",
        "imshow(inter_images, col=inter_images.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3WqC88INrEx"
      },
      "source": [
        "## Caffe model, function to plot results from its results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHltarooOT4R"
      },
      "source": [
        "# Function used to plot results from Caffe model\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_caffe():\n",
        "    labels = ['-4', '-3', '-2', '-1', '1', '2', '3', '4']\n",
        "\n",
        "    attrs_cum = np.zeros((8, 40))\n",
        "    with open('./result/demo_result_int.list') as f:\n",
        "        for idx,line in enumerate(f):\n",
        "            attrs = line.split()[1:]\n",
        "            if idx%9 == 0:\n",
        "                base_attrs = attrs\n",
        "                title = line.split('/')[4]\n",
        "            else:\n",
        "                for idx2,attr in enumerate(attrs):\n",
        "                    if attr != base_attrs[idx2]:\n",
        "                        attrs_cum[(idx%9)-1, idx2] += 1\n",
        "            if idx%117 == 116:\n",
        "                fig,ax = plt.subplots(1)\n",
        "                [a,b,c,d,e,f,g,h] = ax.plot(attrs_cum.T/13, label=labels);\n",
        "                ax.legend([a,b,c,d,e,f,g,h], labels, loc=1)\n",
        "                ax.set_xticks(np.linspace(0,40,41), minor=True)\n",
        "                ax.grid(True, linestyle='--', linewidth=1, which='major')\n",
        "                ax.grid(True, linestyle='--', linewidth=1, which='minor', alpha=0.5)\n",
        "                plt.title(title)\n",
        "                fig.show()\n",
        "                attrs_cum = np.zeros((8, 40))\n",
        "\n",
        "    features = np.array(['5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive', 'Bags_Under_Eyes', 'Bald',\n",
        "    'Bangs', 'Big_Lips', 'Big_Nose', 'Black_Hair', 'Blond_Hair', 'Blurry',\n",
        "    'Brown_Hair', 'Bushy_Eyebrows', 'Chubby', 'Double_Chin', 'Eyeglasses', 'Goatee',\n",
        "    'Gray_Hair', 'Heavy_Makeup', 'High_Cheekbones', 'Male', 'Mouth_Slightly_Open',\n",
        "    'Mustache', 'Narrow_Eyes', 'No_Beard', 'Oval_Face', 'Pale_Skin', 'Pointy_Nose',\n",
        "    'Receding_Hairline', 'Rosy_Cheeks', 'Sideburns', 'Smiling', 'Straight_Hair',\n",
        "    'Wavy_Hair', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Lipstick',\n",
        "    'Wearing_Necklace', 'Wearing_Necktie', 'Young'])\n",
        "\n",
        "    print(np.where(features == 'Smiling'))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}