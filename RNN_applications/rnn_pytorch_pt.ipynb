{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S05A01_1 - RNN Forward.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sk3TpLb8DhT",
        "colab_type": "text"
      },
      "source": [
        "# Preâmbulo\n",
        "\n",
        "Imports básicos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heM-v_e475r8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Basic imports.\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xONfLuDyDvSB",
        "colab_type": "code",
        "outputId": "967b2c2e-210a-4638-87a4-3cd21bf6878b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Setting predefined arguments.\n",
        "args = {}\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    args['device'] = torch.device('cuda')\n",
        "else:\n",
        "    args['device'] = torch.device('cpu')\n",
        "\n",
        "print(args['device'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNqxMX9v8YC-",
        "colab_type": "text"
      },
      "source": [
        "## Relembrando Redes Neurais Feed-Forward\n",
        "Um perceptron comum, que integra uma Rede Neural Feed-Forward, realiza a seguinte operaçao:\n",
        "\n",
        "\\begin{align*}\n",
        "f(X) = \\sigma (W_{xh}X + b_h),\n",
        "\\end{align*}\n",
        "\n",
        "sendo \n",
        "- $X \\in R^{b \\times d}$ um mini batch de $b$ amostras com $d$ dimensões;\n",
        "- $W_{xh} \\in R^{d \\times h}$ a matriz de pesos, com $h$ sendo a dimensionalidade da saída desejada;\n",
        "- $b_h \\in R^{1 \\times h}$ o viés;\n",
        "- $\\sigma$ a ativação não linear.\n",
        "\n",
        "O resultado é $f(X) \\in R^{b \\times h}$, podendo tanto ser a feature intermediária que alimentará camadas futuras, ou a saída final do modelo. \n",
        "\n",
        "O código a seguir implementa um forward do zero de uma camada feed forward simples.  A nomenclatura *hidden_size* representa a dimensionalidade da saída da camada.\n",
        "\n",
        "Hiperparâmetros\n",
        "- batch size = 10\n",
        "- input size = 100\n",
        "- hidden size = 512\n",
        "- função de ativação = tanh"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVvQhuKw8ZLI",
        "colab_type": "code",
        "outputId": "a36e1bb5-9f50-45cf-8b01-ff9fa9b5baa5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def init_weights(input_size, hidden_size):\n",
        "  params = {}\n",
        "  params['Wxh'] = np.random.randn(input_size, hidden_size)\n",
        "  params['bh']  = np.random.randn(1, hidden_size)\n",
        "  \n",
        "  return params\n",
        "\n",
        "def forward(X, params):\n",
        "  return np.tanh(np.dot(X, params['Wxh']) + params['bh'])\n",
        "\n",
        "batch_size  = 10\n",
        "input_size  = 100\n",
        "hidden_size = 512\n",
        "\n",
        "## Generate random batch \n",
        "X = np.random.randn(batch_size, input_size)\n",
        "\n",
        "## Init weights\n",
        "params = init_weights(input_size, hidden_size)\n",
        "\n",
        "## Forward\n",
        "output = forward(X, params)\n",
        "print(output.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXfFTIPN94iD",
        "colab_type": "text"
      },
      "source": [
        "## Redes Recorrentes - *Forward from Scratch*\n",
        "\n",
        "Uma rede neural recorrente, por outro lado, é uma função não só de $X$, mas também de um *hidden state* $H \\in R^{b \\times h}$, que representa a memória interna da unidade recorrente. Em outras palavras, a unidade recorrente é uma função da entrada atual $X_t$ e do conhecimento prévio acumulado $H_{t-1}$, com $t$ representando os timesteps. Para tal, adiciona-se uma matriz de pesos $W_{hh} \\in R^{h \\times h}$ que opera com a memória interna de timesteps anteriores, ou seja\n",
        "\n",
        "\\begin{align*}\n",
        "f(X_t, H_{t-1}) = \\sigma (W_{xh}X_t + W_{hh}H_{t-1} + b_h).\n",
        "\\end{align*}\n",
        "\n",
        "Esse é um processo **iterativo**, que deve ser executado com todos os elementos $X_t$ da sequência de entrada $X = \\{X_1, X_2,..., X_n\\}$, sendo $n$ o tamanho da sequência. O forward produz o conjunto de *hidden states* $H = \\{H_1, H_2, ..., H_n\\}$.\n",
        "\n",
        "**Atenção:** Na primeira iteração, o *hidden state* $H_0$ deve ser inicializado. Existem vários métodos de inicialização, sendo os mais comuns inicialização aleatória ou com zeros. Veja a seguir uma ilustração do processo iterativo na versão compacta (esquerda) e desenrolada (direita):\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=13lVkpXH5GqE8YjgpGyo8fFzRBKqPder7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SAQ5Yxg-gnN",
        "colab_type": "text"
      },
      "source": [
        "A célula a seguir contém o forward iterativo de uma camada recorrente simples. A saída apresenta é o $H \\in R^{n \\times b \\times h}$ contendo todos os hidden states produzidos ao longo das iterações. \n",
        "\n",
        "**Atenção:** A entrada de uma unidade recorrente **não** segue mais o padrão $(B,C,H,W)$ que estávamos acostumados, seguindo agora o padrão $(N,B,D)$ sendo $N$ o tamanho da sequência e $D$ a dimensionalidade de cada elemento da sequência.\n",
        "\n",
        "Hiperparâmetros\n",
        "- **sequence length** = 50\n",
        "- batch size = 10\n",
        "- input size = 100\n",
        "- hidden size = 512\n",
        "- função de ativação = tanh"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyvMkrH096Gc",
        "colab_type": "code",
        "outputId": "79de665c-3bdd-4713-d2a7-fb2c7a046541",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def init_weights(input_size, hidden_size):\n",
        "  params = {}\n",
        "  params['Wxh'] = np.random.randn(input_size, hidden_size)\n",
        "  params['Whh'] = np.random.randn(hidden_size, hidden_size)\n",
        "  params['bh']  = np.random.randn(1, hidden_size)\n",
        "  \n",
        "  return params\n",
        "\n",
        "def forward(X, params):\n",
        "  \n",
        "  # Initialize hidden state for first iteration\n",
        "  H = np.random.randn(batch_size, hidden_size)\n",
        "  \n",
        "  # Iterative forward\n",
        "  hidden_states = []\n",
        "  for x in X:\n",
        "    H = np.tanh(np.dot(x, params['Wxh']) + np.dot(H, params['Whh']) + params['bh'] ) \n",
        "    hidden_states.append(H)\n",
        "    \n",
        "  return np.array(hidden_states) # output (seq_len, batch_size, hidden_size)\n",
        "\n",
        "seq_len     = 50\n",
        "batch_size  = 10\n",
        "input_size  = 100\n",
        "hidden_size = 512\n",
        "\n",
        "## Generate random batch \n",
        "X = np.random.randn(seq_len, batch_size, input_size)\n",
        "\n",
        "## Init weights\n",
        "params = init_weights(input_size, hidden_size)\n",
        "\n",
        "## Forward\n",
        "output = forward(X, params)\n",
        "print(output.shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50, 10, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6gKU7tcbfdc",
        "colab_type": "text"
      },
      "source": [
        "## Pytorch RNNCell\n",
        "\n",
        "Documentação: https://pytorch.org/docs/stable/nn.html#torch.nn.RNNCell\n",
        "\n",
        "Essa camada realiza a operação de uma célula básica de RNN, que já conhecemos pela definição a seguir\n",
        "\n",
        "\\begin{align*}\n",
        "H_t = f(X_t, H_{t-1}) = \\sigma (W_{xh}X_t + W_{hh}H_{t-1} + b_h).\n",
        "\\end{align*}\n",
        "\n",
        "O forward é realizado de forma idêntica ao que fizemos anteriormente, iterando na sequência para operar com cada um dos elementos. A saída a cada timestep $t$ é a memória atualizada $H_t$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OliwL4HuibAM",
        "colab_type": "code",
        "outputId": "9de6a4c9-6793-40ee-f3a8-c4ae13d97714",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "class RNN(nn.Module):\n",
        "  \n",
        "  def __init__(self, input_size, hidden_size, batch_size):\n",
        "    super(RNN, self).__init__()\n",
        "    \n",
        "    self.hidden_size = hidden_size\n",
        "    self.batch_size  = batch_size\n",
        "    \n",
        "    self.rnn = nn.RNNCell(input_size, hidden_size)\n",
        "\n",
        "  def forward(self, X):\n",
        "\n",
        "    # Initialize hidden state for first iteration\n",
        "    H = torch.randn(self.batch_size, self.hidden_size).to(args['device'])\n",
        "\n",
        "    # Iterative forward\n",
        "    hidden_states = []\n",
        "    for x in X:\n",
        "      H = self.rnn(x, H)\n",
        "      hidden_states.append(H)\n",
        "\n",
        "    return torch.stack(hidden_states) # output (seq_len, batch_size, hidden_size)\n",
        "\n",
        "net = RNN(input_size, hidden_size, batch_size).to(args['device'])\n",
        "  \n",
        "seq_len     = 50\n",
        "batch_size  = 10\n",
        "input_size  = 100\n",
        "hidden_size = 512\n",
        "\n",
        "## Generate random batch \n",
        "X = torch.randn(seq_len, batch_size, input_size).to(args['device'])\n",
        "\n",
        "## Forward\n",
        "output = net(X)\n",
        "print(output.size())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([50, 10, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wl1SAtKUo5S1",
        "colab_type": "text"
      },
      "source": [
        "## Pytorch RNN \n",
        "\n",
        "Documentação: https://pytorch.org/docs/stable/nn.html#torch.nn.RNN\n",
        "\n",
        "A camada RNN encapsula todo o processamento na dimensão do tempo, **eliminando a necessidade de realizar um loop explícito na implementação do forward**. A implementação dessa camada no pytorch espera uma entrada na forma ```(seq_len, batch_size, input_size)```, apresentando uma saída na forma  ```(seq_len, batch_size, hidden_size)```. Isso pode ser alterado definindo o parâmetro ```batch_first = True``` ao instanciar a camada, artifício útil quando se utiliza o DataLoader para carregamento dos dados.\n",
        "\n",
        "Outra vantagem é a possibilidade de implementar **múltiplas camadas ao mesmo tempo** com a simples definição de um parâmetro inteiro ```num_layers``` ao instanciar a RNN.\n",
        "\n",
        "### Multilayer RNN\n",
        "\n",
        "#### Com a camada RNNCell\n",
        "``` python\n",
        "# Definição de camadas\n",
        "self.rnn1 = nn.RNNCell(input_size, hidden_size)\n",
        "self.rnn2 = nn.RNNCell(hidden_size, hidden_size)\n",
        "\n",
        "# Forward\n",
        "h_layer1 = torch.randn(self.batch_size, self.hidden_size)\n",
        "h_layer2 = torch.randn(self.batch_size, self.hidden_size)\n",
        "\n",
        "for x in X:\n",
        "  h_layer1 = self.rnn1(x, h_layer1)\n",
        "  h_layer2 = self.rnn2(h_layer1, h_layer2)\n",
        "```\n",
        "\n",
        "#### Com a camada RNN\n",
        "\n",
        "``` python\n",
        "# Definição de camadas\n",
        "self.rnn = nn.RNN(input_size, hidden_size, num_layers)\n",
        "\n",
        "# Forward\n",
        "h = torch.randn(self.num_layers, self.batch_size, self.hidden_size)\n",
        "output, h = self.rnn(X, h) \n",
        "# output size = (seq_len, batch_size, hidden_size)\n",
        "# h size = (num_layers, batch_size, hidden_size)\n",
        "```\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=16E2NJxWY14jj3Qq5RaUvodqpd3Sd-ECZ)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhWe-vVumw8x",
        "colab_type": "code",
        "outputId": "aa1cdc5c-426b-407e-ed1a-8ecdf980051a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "class RNN(nn.Module):\n",
        "  \n",
        "  def __init__(self, input_size, hidden_size, batch_size, num_layers):\n",
        "    super(RNN, self).__init__()\n",
        "    \n",
        "    self.hidden_size = hidden_size\n",
        "    self.batch_size  = batch_size\n",
        "    self.num_layers  = num_layers\n",
        "    \n",
        "    self.rnn = nn.RNN(input_size, hidden_size, num_layers)\n",
        "\n",
        "  def forward(self, X):\n",
        "\n",
        "    # Initialize hidden state for first iteration\n",
        "    H = torch.randn(self.num_layers, self.batch_size, self.hidden_size).to(args['device'])\n",
        "\n",
        "    # Iterative forward\n",
        "    outputs, H = self.rnn(X, H)\n",
        "    print('Output size:', outputs.size(), \"\\nHidden Size:\", H.size())\n",
        "    \n",
        "    return outputs\n",
        "\n",
        "net = RNN(input_size, hidden_size, batch_size, num_layers).to(args['device'])\n",
        "\n",
        "seq_len     = 50\n",
        "batch_size  = 10\n",
        "input_size  = 100\n",
        "hidden_size = 512\n",
        "num_layers  = 2\n",
        "\n",
        "## Generate random batch \n",
        "X = torch.randn(seq_len, batch_size, input_size).to(args['device'])\n",
        "\n",
        "## Forward\n",
        "output = net(X)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output size: torch.Size([50, 10, 512]) \n",
            "Hidden Size: torch.Size([2, 10, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}