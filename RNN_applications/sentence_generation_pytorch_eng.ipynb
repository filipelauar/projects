{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentence_generation_pytorch_eng.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook you will find a pytorch implementation from scratch of RNN, LSTM and GRU and also a code to generate sentences.\n",
        "\n",
        "There is also the implementation of the beam search and also the pNucleos sampling strategy.\n",
        "\n",
        "Hope it can be helpfull for you guys!\n",
        "\n"
      ],
      "metadata": {
        "id": "S9OKOemUvMZg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZiWEXwmXtkZ"
      },
      "source": [
        "import sys\n",
        "import unicodedata\n",
        "import string\n",
        "from typing import List\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import re\n",
        "import math\n",
        "import copy\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from itertools import chain\n",
        "from torch.nn.functional import embedding, one_hot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvjwc9GGUNjH"
      },
      "source": [
        "#Text loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oeybVKAToUS"
      },
      "source": [
        "## Token de padding (BLANK)\n",
        "PAD_IX = 0\n",
        "## Token de fin de séquence\n",
        "EOS_IX = 1\n",
        "\n",
        "LETTRES = string.ascii_letters + string.punctuation + string.digits + ' '\n",
        "id2lettre = dict(zip(range(2, len(LETTRES)+2), LETTRES))\n",
        "id2lettre[PAD_IX] = '<PAD>' ##NULL CHARACTER\n",
        "id2lettre[EOS_IX] = '<EOS>'\n",
        "lettre2id = dict(zip(id2lettre.values(),id2lettre.keys()))\n",
        "\n",
        "\n",
        "def normalize(s):\n",
        "    \"\"\" enlève les accents et les caractères spéciaux\"\"\"\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s) if  c in LETTRES)\n",
        "\n",
        "def string2code(s):\n",
        "    \"\"\"prend une séquence de lettres et renvoie la séquence d'entiers correspondantes\"\"\"\n",
        "    return torch.tensor([lettre2id[c] for c in normalize(s)])\n",
        "\n",
        "def code2string(t):\n",
        "    \"\"\" prend une séquence d'entiers et renvoie la séquence de lettres correspondantes \"\"\"\n",
        "    if type(t) !=list:\n",
        "        t = t.tolist()\n",
        "    return ''.join(id2lettre[i] for i in t)\n",
        "\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text: str, *, maxsent=None, maxlen=None):\n",
        "        \"\"\"  Dataset pour les tweets de Trump\n",
        "            * fname : nom du fichier\n",
        "            * maxsent : nombre maximum de phrases.\n",
        "            * maxlen : longueur maximale des phrases.\n",
        "        \"\"\"\n",
        "        maxlen = maxlen or sys.maxsize\n",
        "        self.phrases = [re.sub(' +',' ',p[:maxlen]).strip() +\".\" for p in text.split(\".\") if len(re.sub(' +',' ',p[:maxlen]).strip())>0]\n",
        "        if maxsent is not None:\n",
        "            self.phrases=self.phrases[:maxsent]\n",
        "        self.maxlen = max([len(p) for p in self.phrases])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.phrases)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return string2code(self.phrases[i])\n",
        "    \n",
        "\n",
        "def pad_collate_fn(samples: List[List[int]]):\n",
        "    #  TODO:  Renvoie un batch à partir d'une liste de listes d'indexes (de phrases) qu'il faut padder.\n",
        "    \n",
        "    lenght = max([len(x) for x in samples])\n",
        "    out = torch.zeros((len(samples), lenght+1))\n",
        "    \n",
        "    \n",
        "    for i, samp in enumerate(samples):\n",
        "        out[i, :(samp.shape[0]+1)] = torch.cat((samp, torch.tensor([EOS_IX])))\n",
        "\n",
        "        \n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBqBwOuiUKzE"
      },
      "source": [
        "#Generate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rQ78MUHTwqg"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# p_nucleus\n",
        "def p_nucleus(words_proba, alpha: float):\n",
        "    #  TODO:  Implémentez le Nucleus sampling ici (pour un état s)\n",
        "        \n",
        "    args = torch.argsort(words_proba,descending=True)\n",
        "    tot = 0\n",
        "    res = torch.zeros(words_proba.shape).to(device)\n",
        "    for i in args:\n",
        "        tot += words_proba[i]\n",
        "        res[i] = words_proba[i]\n",
        "        if tot >= alpha:\n",
        "            break\n",
        "    return res/tot\n",
        "\n",
        "#Deterministic and probabilistic generate\n",
        "def generate(rnn, emb, eos, model_name='RNN', start=\"I\", maxlen=200, determinist=False, nucleus=False, alpha=0.95):\n",
        "    \"\"\"  Fonction de génération (l'embedding et le decodeur être des fonctions du rnn). Initialise le réseau avec start (ou à 0 si start est vide) et génère une séquence de longueur maximale 200 ou qui s'arrête quand eos est généré.\n",
        "        * rnn : le réseau\n",
        "        * emb : la couche d'embedding\n",
        "        * decoder : le décodeur\n",
        "        * eos : ID du token end of sequence\n",
        "        * start : début de la phrase\n",
        "        * maxlen : longueur maximale\n",
        "    \"\"\"\n",
        "    #  TODO:  Implémentez la génération à partir du RNN, et d'une fonction decoder qui renvoie les logits (logarithme de probabilité à une constante près, i.e. ce qui vient avant le softmax) des différentes sorties possibles\n",
        "    \n",
        "    print(model_name)\n",
        "    x = [0] if start==\"\" else [string2code(start)]\n",
        "    #x = [string2code('I')]\n",
        "    sentence = start\n",
        "\n",
        "    h = torch.zeros((1,latent_space)).to(device)\n",
        "    if model_name == 'LSTM':\n",
        "      c = torch.zeros((1,latent_space)).to(device)\n",
        "      \n",
        "    for _ in range(maxlen - len(start)):\n",
        "      if model_name == 'RNN':\n",
        "        x_one_hot = torch.tensor(one_hot(torch.tensor(x[-1]),len(lettre2id)),dtype=torch.float).reshape((1,-1)).to(device)\n",
        "      else:\n",
        "        x_one_hot = torch.tensor(one_hot(torch.tensor(x[-1]),len(lettre2id)),dtype=torch.float).reshape((1,1,-1)).to(device)\n",
        "        \n",
        "      x_emb = emb(x_one_hot)\n",
        "      if model_name != 'RNN':\n",
        "        x_emb = x_emb[0]\n",
        "\n",
        "      if model_name == 'LSTM':\n",
        "        h, c = model.one_step(x_emb, h, c)\n",
        "      else:\n",
        "        h = model.one_step(x_emb, h)\n",
        "\n",
        "      p_x = nn.functional.softmax(model.decode(h)[0],dim=0)\n",
        "\n",
        "      if nucleus:\n",
        "        p_x = p_nucleus(p_x, alpha)\n",
        "      if determinist:\n",
        "        word_idx = torch.argmax(p_x).item()\n",
        "      else:\n",
        "        word_idx = torch.multinomial(p_x, 1).item() # predicting the next word\n",
        "      if word_idx==eos:\n",
        "          return sentence\n",
        "          \n",
        "      sentence += id2lettre[word_idx]\n",
        "      x.append(word_idx)\n",
        "          \n",
        "    return sentence\n",
        "\n",
        "#Beam search\n",
        "\n",
        "def generate_beam(rnn, emb, eos, k, model_name='RNN', start=\"\", maxlen=200, nucleus=False, alpha=0.95):\n",
        "    \"\"\"  Fonction de génération (l'embedding et le decodeur être des fonctions du rnn). Initialise le réseau avec start (ou à 0 si start est vide) et génère une séquence de longueur maximale 200 ou qui s'arrête quand eos est généré.\n",
        "        * rnn : le réseau\n",
        "        * emb : la couche d'embedding\n",
        "        * decoder : le décodeur\n",
        "        * eos : ID du token end of sequence\n",
        "        * start : début de la phrase\n",
        "        * maxlen : longueur maximale\n",
        "    \"\"\"\n",
        "    #  TODO:  Implémentez la génération à partir du RNN, et d'une fonction decoder qui renvoie les logits (logarithme de probabilité à une constante près, i.e. ce qui vient avant le softmax) des différentes sorties possibles\n",
        "    \n",
        "    print(model_name)\n",
        "    x = [0] if start==\"\" else [string2code(start)]\n",
        "    #x = [string2code('I')]\n",
        "    sentence = start\n",
        "\n",
        "    h = torch.zeros((1,latent_space)).to(device)\n",
        "    if model_name == 'LSTM':\n",
        "      c = torch.zeros((1,latent_space)).to(device)\n",
        "      \n",
        "    for _ in range(maxlen - len(start)):\n",
        "      if model_name == 'RNN':\n",
        "        x_one_hot = torch.tensor(one_hot(torch.tensor(x[-1]),len(lettre2id)),dtype=torch.float).reshape((1,-1)).to(device)\n",
        "      else:\n",
        "        x_one_hot = torch.tensor(one_hot(torch.tensor(x[-1]),len(lettre2id)),dtype=torch.float).reshape((1,1,-1)).to(device)\n",
        "        \n",
        "      x_emb = emb(x_one_hot)\n",
        "      if model_name != 'RNN':\n",
        "        x_emb = x_emb[0]\n",
        "\n",
        "      if model_name == 'LSTM':\n",
        "        h, c = model.one_step(x_emb, h, c)\n",
        "      else:\n",
        "        h = model.one_step(x_emb, h)\n",
        "\n",
        "      p_x = nn.functional.softmax(model.decode(h)[0],dim=0)\n",
        "\n",
        "      if nucleus:\n",
        "        p_x = p_nucleus(p_x, alpha)\n",
        "\n",
        "      #Beam search part\n",
        "      topK_idx = torch.argsort(p_x, descending=True)[:k]\n",
        "      best_idx, best_proba = 0, 0\n",
        "      for idx in topK_idx:\n",
        "          if model_name == 'RNN':\n",
        "            idx_one_hot = torch.tensor(one_hot(torch.tensor(idx),len(lettre2id)),dtype=torch.float).reshape((1,-1)).to(device)\n",
        "          else:\n",
        "            idx_one_hot = torch.tensor(one_hot(torch.tensor(idx),len(lettre2id)),dtype=torch.float).reshape((1,1,-1)).to(device)\n",
        "            \n",
        "          x_emb = emb(idx_one_hot)\n",
        "          if model_name != 'RNN':\n",
        "            x_emb = x_emb[0]\n",
        "\n",
        "          if model_name == 'LSTM':\n",
        "            h1, c1 = rnn.one_step(x_emb, h, c)\n",
        "          else:\n",
        "            h1 = rnn.one_step(x_emb, h)\n",
        "          out = rnn.decode(h1)\n",
        "          best_proba_t1 = nn.functional.softmax(out[0],dim=0).max()\n",
        "          if p_x[idx.item()]*best_proba_t1 > best_proba:\n",
        "              best_proba = p_x[idx]*best_proba_t1\n",
        "              best_idx = idx\n",
        "\n",
        "      word_idx = best_idx.item()\n",
        "      if word_idx==eos:\n",
        "          return sentence\n",
        "      sentence = sentence + id2lettre[word_idx]\n",
        "      x.append(word_idx)\n",
        "          \n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpcxSe1yURFm"
      },
      "source": [
        "#Models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I tried my best to make a clean implementation. In the LSTM and GRU implementations, I use a \"trick\" that is train all the gates together and then split them. So I train them all together and then use gate.chunk() to split them to it's own gate."
      ],
      "metadata": {
        "id": "knjT8loxulFW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPPYeZRaT4K2"
      },
      "source": [
        "def maskedCrossEntropy(output: torch.Tensor, target: torch.LongTensor, padcar: int):\n",
        "    \"\"\"\n",
        "    :param output: Tenseur length x batch x output_dim,\n",
        "    :param target: Tenseur length x batch\n",
        "    :param padcar: index du caractere de padding\n",
        "    \"\"\"\n",
        "    #  TODO:  Implémenter maskedCrossEntropy sans aucune boucle, la CrossEntropy qui ne prend pas en compte les caractères de padding.\n",
        "    mask = torch.where(target == padcar, 0, 1)\n",
        "    loss = CrossEntropyLoss(reduce=None)\n",
        "    \n",
        "    return torch.sum(loss(output,target)*mask)/mask.sum()\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size  = output_size\n",
        "        self.input_size = input_size\n",
        "\n",
        "        self.encode = nn.Linear(input_size+hidden_size, hidden_size)\n",
        "        self.decode_latent = nn.Linear(hidden_size, output_size)\n",
        "    \n",
        "    def decode(self,h):\n",
        "        return self.decode_latent(h)\n",
        "\n",
        "    def predict(self,outs):\n",
        "        return torch.argmax(torch.softmax(outs, dim=0), dim=1)\n",
        "    \n",
        "    def predict_proba(self,outs):\n",
        "        #return torch.argsort(torch.softmax(outs, dim=1), descending = True)[:k]\n",
        "        return torch.softmax(outs, dim=0)\n",
        "        \n",
        "    def one_step(self, x, h):\n",
        "        x_h = torch.cat((x, h), axis=1)\n",
        "        return torch.sigmoid(self.encode(x_h))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Initialize hidden state for first iteration\n",
        "        h = torch.zeros((x.shape[0], self.hidden_size)).to(x.device)\n",
        "        hidden_states = []\n",
        "\n",
        "        # Iterative forward\n",
        "        for i in range(x.shape[1]):\n",
        "            h = self.one_step(x[:,i], h)\n",
        "            hidden_states.append(h)\n",
        "            \n",
        "        return torch.stack(hidden_states)\n",
        "\n",
        "\n",
        "class LSTMCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "\n",
        "        super(LSTMCell, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.input_size = input_size\n",
        "        \n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.tanh = nn.Tanh()\n",
        "        \n",
        "        #W corresponds to Wf, Wi, Wo and Wc, and the same for U.\n",
        "        \n",
        "        self.W = nn.Linear(input_size, 4 * hidden_size, bias=True)\n",
        "        self.U = nn.Linear(hidden_size, 4 * hidden_size, bias=True)\n",
        "\n",
        "\n",
        "    def forward(self, x, h, c):\n",
        "        \n",
        "        gates = self.W(x) + self.U(h)\n",
        "\n",
        "        # Get gates (i_t, f_t, g_t, o_t)\n",
        "        input_gate, forget_gate, cell_gate, output_gate = gates.chunk(4, 1)\n",
        "\n",
        "        i_t = self.sigmoid(input_gate)\n",
        "        f_t = self.sigmoid(forget_gate)\n",
        "        g_t = self.tanh(cell_gate)\n",
        "        o_t = self.sigmoid(output_gate)\n",
        "\n",
        "        cy = c * f_t + i_t * g_t\n",
        "\n",
        "        hy = o_t * self.tanh(cy)\n",
        "\n",
        "        return hy, cy\n",
        "    \n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(LSTM, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.output_size = output_size\n",
        "\n",
        "\n",
        "        self.rnnCell = LSTMCell(self.input_size, self.hidden_size)\n",
        "        \n",
        "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
        "  \n",
        "    def one_step(self, x, h, c):\n",
        "        h, c = self.rnnCell(x, h, c)\n",
        "        return h, c\n",
        "\n",
        "    def decode(self,h):\n",
        "        return self.fc(h)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        h = torch.zeros(x.shape[0], self.hidden_size).to(x.device)\n",
        "        c = torch.zeros(x.shape[0], self.hidden_size).to(x.device)\n",
        "        \n",
        "        hidden_states = []\n",
        "        c_states = []\n",
        "        \n",
        "        #for all words/time stemps\n",
        "        for t in range(x.shape[1]):\n",
        "            #If you want more than 1 layer\n",
        "            for layer in range(self.num_layers):\n",
        "                h, c = self.one_step(x[:,t], h, c)\n",
        "                hidden_states.append(h)\n",
        "                c_states.append(c)\n",
        "        \n",
        "        return torch.stack((torch.stack(hidden_states),torch.stack(c_states)))\n",
        "\n",
        "\n",
        "class GRUCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "\n",
        "        super(GRUCell, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.input_size = input_size\n",
        "        \n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.tanh = nn.Tanh()\n",
        "        \n",
        "        #W corresponds to Wz, Wr and Wh, and the same for U.\n",
        "        \n",
        "        self.W = nn.Linear(input_size, 3 * hidden_size, bias=True)\n",
        "        self.U = nn.Linear(hidden_size, 3 * hidden_size, bias=True)\n",
        "\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        \n",
        "        x_t = self.W(x)\n",
        "        h_t = self.U(h)\n",
        "\n",
        "        # As I have Wz, Wr and Wh on W and the same for U, I need to chunk them\n",
        "        x_reset, x_upd, x_new = x_t.chunk(3, 1)\n",
        "        h_reset, h_upd, h_new = h_t.chunk(3, 1)\n",
        "        \n",
        "        #Setting the gates\n",
        "        reset_gate = self.sigmoid(x_reset + h_reset)\n",
        "        update_gate = self.sigmoid(x_upd + h_upd)\n",
        "        new_gate = self.tanh(x_new + (reset_gate * h_new))\n",
        "\n",
        "        #Next state\n",
        "        hy = update_gate * h + (1 - update_gate) * new_gate\n",
        "\n",
        "        return hy\n",
        "\n",
        "class GRU(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(GRU, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.output_size = output_size\n",
        "\n",
        "\n",
        "        self.rnnCell = GRUCell(self.input_size, self.hidden_size)\n",
        "        \n",
        "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def one_step(self, x, h):\n",
        "        h = self.rnnCell(x, h)\n",
        "        return h\n",
        "\n",
        "    def decode(self,h):\n",
        "        return self.fc(h)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        h = torch.zeros(x.shape[0], self.hidden_size).to(x.device)\n",
        "        \n",
        "        hidden_states = []\n",
        "        \n",
        "        #for all words/time stemps\n",
        "        for t in range(x.shape[1]):\n",
        "            #If you want more than 1 layer\n",
        "            for layer in range(self.num_layers):\n",
        "                h = self.one_step(x[:,t], h)\n",
        "                hidden_states.append(h)\n",
        "        \n",
        "        return torch.stack(hidden_states)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGRF7_5nUVWL"
      },
      "source": [
        "# Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activate the google colab GPU or your GPU, otherwise it will take too long to train. Don't forget to choose the model, you can choose between RNN, LSTM and GRU."
      ],
      "metadata": {
        "id": "jIpXm4YwuVmO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jFbwf6PbUhdS",
        "outputId": "499fc655-3912-48d9-cb59-691e68d613d2"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "#device = 'cpu'\n",
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2mm6AXsUZLN"
      },
      "source": [
        "LEN_GENERATION = 20\n",
        "MAX_SENT = 100\n",
        "latent_space = 128 # taille de l'espace latent caché\n",
        "embedding_space = 64\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "#Download and open the data\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "gdd.download_file_from_google_drive(file_id='1aHpZ_ZmER10OMQE2pXvz93algzuxIhMI',\n",
        "                                    dest_path='./trump_full_speech.txt')\n",
        "\n",
        "with open('trump_full_speech.txt','r') as file:\n",
        "    speech = file.read().replace('\\n', '')\n",
        "\n",
        "dataset = TextDataset(speech, maxlen = MAX_SENT)\n",
        "train_set = DataLoader(dataset, collate_fn=pad_collate_fn, batch_size=batch_size)\n",
        "\n",
        "#Choose the model\n",
        "\n",
        "#model, model_name = RNN(embedding_space, latent_space, len(LETTRES)+2).to(device), 'RNN'\n",
        "#model, model_name = GRU(embedding_space, latent_space, 1,  len(LETTRES)+2).to(device), 'GRU'\n",
        "model, model_name = LSTM(embedding_space, latent_space, 1,  len(LETTRES)+2).to(device), 'LSTM'\n",
        "\n",
        "train = True\n",
        "\n",
        "# if model_name in os.listdir():\n",
        "#   model.load_state_dict(torch.load(model_name))\n",
        "#   train=False\n",
        "\n",
        "nepochs = 30\n",
        "#a linear projection from the one hot encoding to the embedding space\n",
        "embedding = nn.Linear(len(lettre2id), embedding_space).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.Adam(chain(model.parameters(), embedding.parameters()),lr=0.003)# it learn both the RNN for generation and the embeding function\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcONVsyhXfkJ",
        "outputId": "d411c6a5-1a55-4fa4-a3a7-131ca437a223"
      },
      "source": [
        "if train:\n",
        "  best_loss = 10000000000\n",
        "  l=[]\n",
        "  for epoch in range(nepochs):\n",
        "      # print(\"epoch\",epoch)\n",
        "      L=0\n",
        "      for i, x in enumerate(train_set):\n",
        "          y = x[:,1:].to(device)\n",
        "          x = x[:,:-1].to(device)\n",
        "          x_one_hot = torch.tensor(one_hot(x.to(torch.int64), len(LETTRES)+2),dtype=torch.float, requires_grad=True)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          x_emb = embedding(x_one_hot)\n",
        "          \n",
        "          if model_name == 'LSTM':\n",
        "            h, c = model(x_emb)\n",
        "          else:\n",
        "            h = model(x_emb)\n",
        "\n",
        "          h = h.transpose(0, 1)\n",
        "          h = h.reshape(-1, h.shape[2])\n",
        "          outs = model.decode(h)\n",
        "\n",
        "          loss = maskedCrossEntropy(outs, y.flatten().long(), padcar = PAD_IX)\n",
        "          with torch.no_grad():\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "              L += loss.item()\n",
        "      print(\"Epoch:\", epoch, \"loss:\", L/i)\n",
        "      if L/i < best_loss:\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        best_loss=L/i\n",
        "      l.append(L/i)\n",
        "\n",
        "\n",
        "  torch.save(best_model_wts, model_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 loss: 3.1437029196665836\n",
            "Epoch: 1 loss: 2.193857729434967\n",
            "Epoch: 2 loss: 1.9736777039674611\n",
            "Epoch: 3 loss: 1.8837000039907603\n",
            "Epoch: 4 loss: 1.7165221067575307\n",
            "Epoch: 5 loss: 1.6205674914213328\n",
            "Epoch: 6 loss: 1.564539936872629\n",
            "Epoch: 7 loss: 1.5233677075459406\n",
            "Epoch: 8 loss: 1.4892666248174815\n",
            "Epoch: 9 loss: 1.4584157168865204\n",
            "Epoch: 10 loss: 1.4289161952642293\n",
            "Epoch: 11 loss: 1.4003177629067347\n",
            "Epoch: 12 loss: 1.3730718332987566\n",
            "Epoch: 13 loss: 1.3467573729845195\n",
            "Epoch: 14 loss: 1.320893631531642\n",
            "Epoch: 15 loss: 1.2960148660036235\n",
            "Epoch: 16 loss: 1.2724103446190174\n",
            "Epoch: 17 loss: 1.2498757793353155\n",
            "Epoch: 18 loss: 1.2286934646276326\n",
            "Epoch: 19 loss: 1.2090358481957362\n",
            "Epoch: 20 loss: 1.1906669048162608\n",
            "Epoch: 21 loss: 1.1732043004952943\n",
            "Epoch: 22 loss: 1.1566907786406004\n",
            "Epoch: 23 loss: 1.1409386877830212\n",
            "Epoch: 24 loss: 1.1258619794478784\n",
            "Epoch: 25 loss: 1.111381817322511\n",
            "Epoch: 26 loss: 1.0975303099705622\n",
            "Epoch: 27 loss: 1.0843378924406493\n",
            "Epoch: 28 loss: 1.0717724882639372\n",
            "Epoch: 29 loss: 1.0598410597214332\n",
            "Epoch: 30 loss: 1.048537701368332\n",
            "Epoch: 31 loss: 1.0376755778606122\n",
            "Epoch: 32 loss: 1.0271953550668864\n",
            "Epoch: 33 loss: 1.0171180642568147\n",
            "Epoch: 34 loss: 1.0073566001195173\n",
            "Epoch: 35 loss: 0.9980381360420814\n",
            "Epoch: 36 loss: 0.9891181244299962\n",
            "Epoch: 37 loss: 0.9805681774249444\n",
            "Epoch: 38 loss: 0.9724096770469959\n",
            "Epoch: 39 loss: 0.9646205558226659\n",
            "Epoch: 40 loss: 0.9571895943238184\n",
            "Epoch: 41 loss: 0.9500773732478802\n",
            "Epoch: 42 loss: 0.9432555047365335\n",
            "Epoch: 43 loss: 0.9367372210209186\n",
            "Epoch: 44 loss: 0.9305092348502233\n",
            "Epoch: 45 loss: 0.9245377733157232\n",
            "Epoch: 46 loss: 0.9187842171925765\n",
            "Epoch: 47 loss: 0.9132492404717666\n",
            "Epoch: 48 loss: 0.9079403785558847\n",
            "Epoch: 49 loss: 0.9028550523978013\n",
            "Epoch: 50 loss: 0.8979758092990289\n",
            "Epoch: 51 loss: 0.8932844331631293\n",
            "Epoch: 52 loss: 0.8887498653852023\n",
            "Epoch: 53 loss: 0.8843846664978907\n",
            "Epoch: 54 loss: 0.8801345297923455\n",
            "Epoch: 55 loss: 0.8760534158119788\n",
            "Epoch: 56 loss: 0.8720789551734924\n",
            "Epoch: 57 loss: 0.8682840053851788\n",
            "Epoch: 58 loss: 0.8645771650167612\n",
            "Epoch: 59 loss: 0.8610275020966163\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "9wrS0YF0TqbU",
        "outputId": "d096f243-d747-4abe-c822-a81784eb02dc"
      },
      "source": [
        "if train:\n",
        "  plt.plot(l)\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.title(model_name)\n",
        "  plt.show();"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ROPvvkx6Sex"
      },
      "source": [
        "#Generate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here you can play with the starting word or sentence and see different results by setting determinist = False."
      ],
      "metadata": {
        "id": "Vg4yzIrutauP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "ZmPJ81cj6T1H",
        "outputId": "556f3327-67e1-4401-eb33-db612c82a182"
      },
      "source": [
        "generate(model, embedding, EOS_IX, model_name, start=\"T\", determinist=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"They're uplane, olly get mrone, vote and Russ, we've seez in this dealth.\""
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUkbVReq6T-Z"
      },
      "source": [
        "# Beam search"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here you can play with the starting letter or sentence, K, the pNucleos... And you can also test different methods."
      ],
      "metadata": {
        "id": "zR-1O1dbtoW2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "V7ns81CY6V6g",
        "outputId": "5a84d9ea-b309-412e-fc0f-33251bb52dc5"
      },
      "source": [
        "generate_beam(model, embedding, EOS_IX, 1, model_name, start=\"H\",  nucleus=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Hillary Clinton is the policies and the policies and the policies and the policies and the policies and the policies and the policies and the policies and the policies and the policies and the policie'"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ]
}