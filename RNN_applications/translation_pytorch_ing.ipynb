{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "translation_pytorch_ing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2DqKLTRy4-K"
      },
      "source": [
        "In this tp you will find an example of translation from english to french using pytorch. You can easily modify the code to perform the translation from french to english."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHWHpLr3-aCS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e832b4ca-f325-4518-9e4c-feef6e7974e1"
      },
      "source": [
        "import logging\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch\n",
        "import unicodedata\n",
        "import string\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "from itertools import chain\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import time\n",
        "import re\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "\n",
        "def normalize(s):\n",
        "    return re.sub(' +',' ', \"\".join(c if c in string.ascii_letters else \" \"\n",
        "         for c in unicodedata.normalize('NFD', s.lower().strip())\n",
        "         if  c in string.ascii_letters+\" \"+string.punctuation)).strip()\n",
        "\n",
        "\n",
        "\n",
        "class Vocabulary:\n",
        "    \"\"\"Permet de gérer un vocabulaire.\n",
        "\n",
        "    En test, il est possible qu'un mot ne soit pas dans le\n",
        "    vocabulaire : dans ce cas le token \"__OOV__\" est utilisé.\n",
        "    Attention : il faut tenir compte de cela lors de l'apprentissage !\n",
        "\n",
        "    Utilisation:\n",
        "\n",
        "    - en train, utiliser v.get(\"blah\", adding=True) pour que le mot soit ajouté\n",
        "      automatiquement\n",
        "    - en test, utiliser v[\"blah\"] pour récupérer l'ID du mot (ou l'ID de OOV)\n",
        "    \"\"\"\n",
        "    PAD = 0\n",
        "    EOS = 1\n",
        "    SOS = 2\n",
        "    OOVID = 3\n",
        "\n",
        "    def __init__(self, oov: bool):\n",
        "        self.oov = oov\n",
        "        self.id2word = [\"PAD\", \"EOS\", \"SOS\"]\n",
        "        self.word2id = {\"PAD\": Vocabulary.PAD, \"EOS\": Vocabulary.EOS, \"SOS\": Vocabulary.SOS}\n",
        "        if oov:\n",
        "            self.word2id[\"__OOV__\"] = Vocabulary.OOVID\n",
        "            self.id2word.append(\"__OOV__\")\n",
        "\n",
        "    def __getitem__(self, word: str):\n",
        "        if self.oov:\n",
        "            return self.word2id.get(word, Vocabulary.OOVID)\n",
        "        return self.word2id[word]\n",
        "\n",
        "    def get(self, word: str, adding=True):\n",
        "        try:\n",
        "            return self.word2id[word]\n",
        "        except KeyError:\n",
        "            if adding:\n",
        "                wordid = len(self.id2word)\n",
        "                self.word2id[word] = wordid\n",
        "                self.id2word.append(word)\n",
        "                return wordid\n",
        "            if self.oov:\n",
        "                return Vocabulary.OOVID\n",
        "            raise\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.id2word)\n",
        "\n",
        "    def getword(self, idx: int):\n",
        "        if idx < len(self):\n",
        "            return self.id2word[idx]\n",
        "        return None\n",
        "\n",
        "    def getwords(self, idx: List[int]):\n",
        "        return [self.getword(i) for i in idx]\n",
        "\n",
        "\n",
        "\n",
        "class TradDataset():\n",
        "    def __init__(self,data,vocOrig,vocDest,adding=True,max_len=10):\n",
        "        self.sentences =[]\n",
        "        for s in tqdm(data.split(\"\\n\")):\n",
        "            if len(s)<1:continue\n",
        "            orig,dest=map(normalize,s.split(\"\\t\")[:2])\n",
        "            if len(orig)>max_len: continue\n",
        "            self.sentences.append((torch.tensor([vocOrig.get(o) for o in orig.split(\" \")]+[Vocabulary.EOS]),torch.tensor([vocDest.get(o) for o in dest.split(\" \")]+[Vocabulary.EOS])))\n",
        "    def __len__(self):return len(self.sentences)\n",
        "    def __getitem__(self,i): return self.sentences[i]\n",
        "    def getSizes(self): return len(vocEng), len(vocFra)\n",
        "\n",
        "\n",
        "\n",
        "def collate(batch):\n",
        "    orig,dest = zip(*batch)\n",
        "    o_len = torch.tensor([len(o) for o in orig])\n",
        "    d_len = torch.tensor([len(d) for d in dest])\n",
        "    return pad_sequence(orig),o_len,pad_sequence(dest),d_len\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "#Download and open the data\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "gdd.download_file_from_google_drive(file_id='1u9wP8BY6GjR94AVqlA3Qq9KR1y_qi27y',\n",
        "                                    dest_path='./en-fra.txt')\n",
        "with open('en-fra.txt') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "lines = [lines[x] for x in torch.randperm(len(lines))]\n",
        "idxTrain = int(0.8*len(lines))\n",
        "\n",
        "vocEng = Vocabulary(True)\n",
        "vocFra = Vocabulary(True)\n",
        "MAX_LEN=100\n",
        "BATCH_SIZE=100\n",
        "\n",
        "#input_size = len(vocEng)\n",
        "#output_size = len(vocFra)\n",
        "num_layers=1\n",
        "embedding_size = 200\n",
        "hidden_size = 164\n",
        "\n",
        "datatrain = TradDataset(\"\".join(lines[:idxTrain]),vocEng,vocFra,max_len=MAX_LEN)\n",
        "datatest = TradDataset(\"\".join(lines[idxTrain:]),vocEng,vocFra,max_len=MAX_LEN)\n",
        "\n",
        "train_loader = DataLoader(datatrain, collate_fn=collate, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(datatest, collate_fn=collate, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "input_size, output_size = datatrain.getSizes()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14009/14009 [00:00<00:00, 19111.42it/s]\n",
            "100%|██████████| 3504/3504 [00:00<00:00, 20278.55it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jItpv6mIwhcD"
      },
      "source": [
        "class encoder(nn.Module):\n",
        "  \n",
        "  def __init__(self,input_size, hidden_size, num_layers, embedding_size):\n",
        "    super(encoder, self).__init__()\n",
        "    \n",
        "    self.num_layers = num_layers\n",
        "    self.hidden_size = hidden_size\n",
        "    \n",
        "    self.embed = nn.Embedding(input_size, embedding_size, padding_idx = Vocabulary.PAD)\n",
        "    self.gru = nn.GRU(embedding_size, hidden_size, num_layers, bidirectional=False)\n",
        "    \n",
        "        \n",
        "  def forward(self, x, batch_sizes):\n",
        "    \n",
        "    x = self.embed(x)\n",
        "    x = torch.nn.utils.rnn.pack_padded_sequence(x.data,batch_sizes,enforce_sorted=False)\n",
        "    _, hn = self.gru(x)\n",
        "    \n",
        "    return hn\n",
        "\n",
        "\n",
        "\n",
        "class decoder(nn.Module):\n",
        "  \n",
        "  def __init__(self,input_size, hidden_size, num_layers, embedding_size, output_size):\n",
        "    super(decoder, self).__init__()\n",
        "    \n",
        "    self.num_layers = num_layers\n",
        "    self.hidden_size = hidden_size\n",
        "    \n",
        "    self.emb = nn.Embedding(output_size, embedding_size, padding_idx = Vocabulary.PAD)\n",
        "    self.gru = nn.GRU(embedding_size, hidden_size, num_layers, bidirectional=False)\n",
        "    self.linear = nn.Linear(hidden_size, output_size)\n",
        "    \n",
        "        \n",
        "  def forward(self, y, batch_sizes, h0, mode='contraint'):\n",
        "    \n",
        "    if mode == 'contraint':\n",
        "        starts = torch.tensor([[Vocabulary.SOS] * y.shape[1]],dtype=torch.int)\n",
        "        starts = starts.to(device)\n",
        "        y = torch.cat((starts,y[:-1])) \n",
        "            \n",
        "        \n",
        "        y = self.emb(y)\n",
        "        y = torch.nn.utils.rnn.pack_padded_sequence(y.data,batch_sizes,enforce_sorted=False)\n",
        "        out, hn = self.gru(y, h0)\n",
        "        output, batch_sizes = torch.nn.utils.rnn.pad_packed_sequence(out)\n",
        "        output = self.linear(output)\n",
        "        return output\n",
        "    else:\n",
        "        res = [self.generate(h0[0][i].unsqueeze(0).unsqueeze(0),len_seq = y.shape[0]) for i in range(h0.shape[1])]\n",
        "        if y.shape[0] - res[0].shape[0] != 0: # to be sure that same size as before\n",
        "            to_add = torch.zeros((y.shape[0] - res[0].shape[0],res[0].shape[1]))\n",
        "            to_add[:,Vocabulary.PAD] = 1\n",
        "            to_add = to_add.to(device)\n",
        "            res[0] = torch.cat([res[0],to_add]) \n",
        "        res = pad_sequence(res,batch_first=True,padding_value=Vocabulary.PAD)\n",
        "\n",
        "        res = res.transpose(0,1)\n",
        "        return res\n",
        "\n",
        "  def generate(self,hidden,len_seq=None,return_hidden = False, start = None):\n",
        "    '''\n",
        "        Take a hidden state and return a sentence of size len_seq or less\n",
        "    '''\n",
        "    i = 0\n",
        "    cur_state = torch.tensor(Vocabulary.SOS) if start is None else torch.tensor(start)\n",
        "    cur_state = cur_state.to(device)\n",
        "    outputs = []\n",
        "    \n",
        "    while (len_seq is None or len_seq != i):\n",
        "        cur_state = self.emb(cur_state)\n",
        "        cur_state = cur_state.unsqueeze(0).unsqueeze(0)\n",
        "        \n",
        "        _, hn = self.gru(cur_state, hidden)\n",
        "        hidden = hn\n",
        "        output = self.linear(hn)[0]\n",
        "        \n",
        "        output_normalized = torch.nn.functional.softmax(output[0],dim = 0)\n",
        "        cur_state = torch.argmax(output_normalized)\n",
        "        outputs += [output[0]]\n",
        "        if cur_state.item() == Vocabulary.EOS:\n",
        "            break\n",
        "\n",
        "        i += 1\n",
        "    if return_hidden:\n",
        "        return torch.stack(outputs), hidden\n",
        "    return torch.stack(outputs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8g47PZZsG1t"
      },
      "source": [
        "nepochs = 100\n",
        "\n",
        "enc = encoder(input_size, hidden_size, num_layers, embedding_size).to(device)\n",
        "dec = decoder(input_size, hidden_size, num_layers, embedding_size, output_size).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = Vocabulary.PAD).to(device)\n",
        "optimizer = torch.optim.Adam(chain(enc.parameters(), dec.parameters()),lr=0.003)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pg8KT561sIKS",
        "outputId": "46433c2b-23fd-48f0-a227-6b2feeee6b19"
      },
      "source": [
        "#l=[]\n",
        "enc.train()\n",
        "dec.train()\n",
        "for epoch in range(nepochs):\n",
        "    L=0\n",
        "    for i, (x, x_size, y, y_size)  in enumerate(train_loader):\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        \n",
        "        hn = enc(x, x_size)\n",
        "        mode = 'contraint' if torch.rand(1).item() else 'non contraint'\n",
        "        out = dec(y, y_size, hn, mode)\n",
        "        loss = criterion(out.reshape(-1, out.shape[2]), y.flatten())\n",
        "\n",
        "        with torch.no_grad():\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            L += loss.item()\n",
        "\n",
        "    print(\"Epoch:\", epoch, \"loss:\", L/i)\n",
        "    l.append(L/i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 loss: 96.78742174421038\n",
            "Epoch: 1 loss: 95.51721354893276\n",
            "Epoch: 2 loss: 95.10952535356795\n",
            "Epoch: 3 loss: 95.94549140930175\n",
            "Epoch: 4 loss: 97.46272065298898\n",
            "Epoch: 5 loss: 97.31585093906948\n",
            "Epoch: 6 loss: 98.82089914594378\n",
            "Epoch: 7 loss: 100.35586825779507\n",
            "Epoch: 8 loss: 98.4081273487636\n",
            "Epoch: 9 loss: 98.75928895132883\n",
            "Epoch: 10 loss: 97.34538083757673\n",
            "Epoch: 11 loss: 96.68699460710798\n",
            "Epoch: 12 loss: 96.95753059387206\n",
            "Epoch: 13 loss: 98.87419662475585\n",
            "Epoch: 14 loss: 101.78673760550363\n",
            "Epoch: 15 loss: 101.01584968566894\n",
            "Epoch: 16 loss: 101.68714076450892\n",
            "Epoch: 17 loss: 102.09367877415248\n",
            "Epoch: 18 loss: 100.94966615949359\n",
            "Epoch: 19 loss: 98.84937433515276\n",
            "Epoch: 20 loss: 100.3335568019322\n",
            "Epoch: 21 loss: 102.4523615700858\n",
            "Epoch: 22 loss: 102.346456854684\n",
            "Epoch: 23 loss: 101.15298658098493\n",
            "Epoch: 24 loss: 100.93251680646624\n",
            "Epoch: 25 loss: 99.68282094682966\n",
            "Epoch: 26 loss: 99.3845333644322\n",
            "Epoch: 27 loss: 96.89693080357142\n",
            "Epoch: 28 loss: 97.77776685442244\n",
            "Epoch: 29 loss: 100.69381321498327\n",
            "Epoch: 30 loss: 100.6005362374442\n",
            "Epoch: 31 loss: 100.91546739850726\n",
            "Epoch: 32 loss: 100.80885222298758\n",
            "Epoch: 33 loss: 100.25779364449637\n",
            "Epoch: 34 loss: 99.46883032662528\n",
            "Epoch: 35 loss: 99.14045818873814\n",
            "Epoch: 36 loss: 98.60798813956124\n",
            "Epoch: 37 loss: 96.85377916608537\n",
            "Epoch: 38 loss: 100.71402756827219\n",
            "Epoch: 39 loss: 100.84873815264021\n",
            "Epoch: 40 loss: 100.20462172372001\n",
            "Epoch: 41 loss: 100.29457212175642\n",
            "Epoch: 42 loss: 99.08332067217145\n",
            "Epoch: 43 loss: 101.851549748012\n",
            "Epoch: 44 loss: 99.85560477120536\n",
            "Epoch: 45 loss: 99.60439186096191\n",
            "Epoch: 46 loss: 100.99300913129534\n",
            "Epoch: 47 loss: 100.39424258640834\n",
            "Epoch: 48 loss: 100.20988360813686\n",
            "Epoch: 49 loss: 102.79000723702568\n",
            "Epoch: 50 loss: 102.80766416277204\n",
            "Epoch: 51 loss: 104.04918474469866\n",
            "Epoch: 52 loss: 102.01695643833705\n",
            "Epoch: 53 loss: 100.02912041800363\n",
            "Epoch: 54 loss: 103.1468202318464\n",
            "Epoch: 55 loss: 104.1342214856829\n",
            "Epoch: 56 loss: 103.68581771850586\n",
            "Epoch: 57 loss: 103.0429643358503\n",
            "Epoch: 58 loss: 104.04513904026577\n",
            "Epoch: 59 loss: 104.25212505885533\n",
            "Epoch: 60 loss: 107.35097203935895\n",
            "Epoch: 61 loss: 109.17934750148228\n",
            "Epoch: 62 loss: 112.10192053658622\n",
            "Epoch: 63 loss: 110.64691881452288\n",
            "Epoch: 64 loss: 114.04300902230399\n",
            "Epoch: 65 loss: 108.96460195268904\n",
            "Epoch: 66 loss: 112.33300328935896\n",
            "Epoch: 67 loss: 114.54775924682617\n",
            "Epoch: 68 loss: 113.51808248247418\n",
            "Epoch: 69 loss: 109.65459529331753\n",
            "Epoch: 70 loss: 111.28016782488142\n",
            "Epoch: 71 loss: 112.46036082676478\n",
            "Epoch: 72 loss: 112.90240151541573\n",
            "Epoch: 73 loss: 112.74159954616002\n",
            "Epoch: 74 loss: 110.78837312970843\n",
            "Epoch: 75 loss: 112.67205113002233\n",
            "Epoch: 76 loss: 111.86286566598075\n",
            "Epoch: 77 loss: 112.40445654732841\n",
            "Epoch: 78 loss: 111.65793026515416\n",
            "Epoch: 79 loss: 110.18661133902414\n",
            "Epoch: 80 loss: 112.29314526149204\n",
            "Epoch: 81 loss: 112.53086678641183\n",
            "Epoch: 82 loss: 112.45596585954938\n",
            "Epoch: 83 loss: 112.78127910069057\n",
            "Epoch: 84 loss: 112.52944701058524\n",
            "Epoch: 85 loss: 109.93695351736886\n",
            "Epoch: 86 loss: 109.75475660051619\n",
            "Epoch: 87 loss: 112.8651917048863\n",
            "Epoch: 88 loss: 113.03014313834053\n",
            "Epoch: 89 loss: 114.27010105678013\n",
            "Epoch: 90 loss: 116.2076684134347\n",
            "Epoch: 91 loss: 114.05585436139788\n",
            "Epoch: 92 loss: 113.1681315285819\n",
            "Epoch: 93 loss: 112.93686403547015\n",
            "Epoch: 94 loss: 114.71249008178711\n",
            "Epoch: 95 loss: 116.6652948651995\n",
            "Epoch: 96 loss: 117.32531264168875\n",
            "Epoch: 97 loss: 115.49883144923619\n",
            "Epoch: 98 loss: 112.59761434282575\n",
            "Epoch: 99 loss: 114.0257394518171\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "zf6OXFLzx20T",
        "outputId": "c6c0d4f4-8e1f-4266-fe80-78165577d18a"
      },
      "source": [
        "plt.plot(l)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ps: I really don't know why my loss is increasing but the results seems acceptable for only 50 epochs. If someone can fix it or get better results, please, let me know!"
      ],
      "metadata": {
        "id": "VVaTylLr-SR9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pcG2YPZsIMq",
        "outputId": "5236e723-cc88-4de5-cee6-a726fdf19843"
      },
      "source": [
        "Ltest = 0\n",
        "enc.eval()\n",
        "dec.eval()\n",
        "for i, (x, x_size, y, y_size)  in enumerate(test_loader):\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    \n",
        "    hn = enc(x, x_size)\n",
        "    mode = 'non contraint'\n",
        "    out = dec(y, y_size, hn, mode)\n",
        "    loss = criterion(out.reshape(-1, out.shape[2]), y.flatten())\n",
        "    Ltest+=loss.item()\n",
        "\n",
        "print(\"Different between test and train losses:\", Ltest/i - l[-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Different between test and train losses: 397.8997566768101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LV5hN_5dsIVC"
      },
      "source": [
        "def generate_sentence(sentence, s_size, enc, dec, string2code, id2lettre, eos, maxlen=200, determinist=True, nucleus=False, alpha=0.95):\n",
        "    \"\"\"  Fonction de génération (l'embedding et le decodeur être des fonctions du rnn). Initialise le réseau avec start (ou à 0 si start est vide) et génère une séquence de longueur maximale 200 ou qui s'arrête quand eos est généré.\n",
        "        * rnn : le réseau\n",
        "        * emb : la couche d'embedding\n",
        "        * decoder : le décodeur\n",
        "        * eos : ID du token end of sequence\n",
        "        * start : début de la phrase\n",
        "        * maxlen : longueur maximale\n",
        "    \"\"\"\n",
        "    #  TODO:  Implémentez la génération à partir du RNN, et d'une fonction decoder qui renvoie les logits (logarithme de probabilité à une constante près, i.e. ce qui vient avant le softmax) des différentes sorties possibles\n",
        "\n",
        "    mode='non contraint'\n",
        "    translation = []\n",
        "\n",
        "    for _ in range(1):\n",
        "      hn = enc(sentence, s_size)\n",
        "      out = dec(sentence, s_size, hn, mode)\n",
        "\n",
        "      out = out.reshape((-1,out.shape[2]))\n",
        "\n",
        "      p_x = nn.functional.softmax(out,dim=1)\n",
        "      \n",
        "      word_idx = torch.argmax(p_x, dim=1)\n",
        "      \n",
        "      for word in word_idx:\n",
        "        if word.item() == 1:\n",
        "          break\n",
        "        translation.append(id2lettre[word.item()])\n",
        "          \n",
        "    return translation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T65UJc9wsIak",
        "outputId": "152e861d-2e63-4ac5-e4a0-a9617c5037f2"
      },
      "source": [
        "x, x_size, y, y_size = next(iter(test_loader))\n",
        "\n",
        "for i in range(10):\n",
        "  sentence = x[:,i].to(device)\n",
        "  s_size = x_size[i].reshape((-1))\n",
        "\n",
        "  eng = ' '.join(vocEng.id2word[id.item()] for id in sentence if id.item() > 1)\n",
        "  print(\"English sentence:\", eng)\n",
        "\n",
        "  fr = generate_sentence(sentence.reshape((-1,1)), s_size, enc, dec, vocFra.word2id, vocFra.id2word, Vocabulary.EOS, maxlen=200, determinist=True)\n",
        "  print(\"French sentence:\", ' '.join(fr))\n",
        "  print('')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English sentence: the switch is on\n",
            "French sentence: bouge toi le bureau\n",
            "\n",
            "English sentence: who called you\n",
            "French sentence: qui vous l as a trahi\n",
            "\n",
            "English sentence: i often read\n",
            "French sentence: j ai un ouvrage de retraite\n",
            "\n",
            "English sentence: you re fearless\n",
            "French sentence: tu es malpoli ouverte\n",
            "\n",
            "English sentence: this is false\n",
            "French sentence: c est un mot\n",
            "\n",
            "English sentence: did you get it\n",
            "French sentence: as tu fait un genie\n",
            "\n",
            "English sentence: i knew too much\n",
            "French sentence: j ai un chien\n",
            "\n",
            "English sentence: this isn t safe\n",
            "French sentence: n as fait ses bagages\n",
            "\n",
            "English sentence: we re biased\n",
            "French sentence: nous nous nous nous nous nous\n",
            "\n",
            "English sentence: she shot him\n",
            "French sentence: le monde ment\n",
            "\n"
          ]
        }
      ]
    }
  ]
}